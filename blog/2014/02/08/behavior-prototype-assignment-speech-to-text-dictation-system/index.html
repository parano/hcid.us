
<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>Behavior Prototype: Speech-to-Text Dictation System - Adventures in HCI+D</title>
	<meta name="author" content="Chaoyu Yang">

	
	<meta name="description" content="This article and its related work is developed by Hadiza Ismaila, Chaoyu Yang, and Chase Chen-Hung Wu. In order to learn about user’s behavior of &hellip;">
	
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="Adventures in HCI+D" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<script async="true" src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	
</head>


<body>
	<header id="header" class="inner"><h1><a href="/">Adventures in HCI+D</a></h1>
<nav id="main-nav"><ul class="main">
	<li><a href="/about">About</a></li>
	<li><a href="/blog/archives">Archives</a></li>
</ul>
</nav>
<nav id="mobile-nav">
	<div class="alignleft menu">
		<a class="button">Menu</a>
		<div class="container"><ul class="main">
	<li><a href="/about">About</a></li>
	<li><a href="/blog/archives">Archives</a></li>
</ul>
</div>
	</div>
	<div class="alignright search">
		<a class="button"></a>
		<div class="container">
			<form action="http://google.com/search" method="get">
				<input type="text" name="q" results="0">
				<input type="hidden" name="q" value="site:hcid.us">
			</form>
		</div>
	</div>
</nav>
<nav id="sub-nav" class="alignright">
	<div class="social">
		
		
		
		<a class="twitter" href="http://twitter.com/paranoyang" title="Twitter">Twitter</a>
		
		
		<a class="github" href="https://github.com/parano" title="GitHub">GitHub</a>
		
    
		
		
		
		
		
		<a class="rss" href="/atom.xml" title="RSS">RSS</a>
		
    
	</div>
	<form class="search" action="http://google.com/search" method="get">
		<input class="alignright" type="text" name="q" results="0">
		<input type="hidden" name="q" value="site:hcid.us">
	</form>
</nav>

</header>
	
		
	
	<div id="content" class="inner"><article class="post">
	<h2 class="title">Behavior Prototype: Speech-to-Text Dictation System</h2>
	<div class="entry-content"><blockquote><p>This article and its related work is developed by Hadiza Ismaila, Chaoyu Yang, and Chase Chen-Hung Wu.</p></blockquote>

<p>In order to learn about user’s behavior of intuitive verbal commands on speech-to-text system, we designed and implemented a rapid behavioral prototype for user testing. Built by Google Drive with a person playing the speech-to-text system (a.k.a. Wizard of Oz), our rapid prototype was tested and modified for several rounds with approaches such as simple text input, editing, error handling, and cursor moving. We summarized and organized the result into different categories of interactive patterns, and came up with valuable learnings and insights from our result analysis.</p>

<div class="video-container">
<iframe width="640" height="360" src="//www.youtube.com/embed/p3B-Bi3ZO8g" frameborder="0" allowfullscreen></iframe>
</div>




<!-- more -->


<h2>Background</h2>

<p><img class="right" src="/images/prototyping/a4/draft.png" width="250"></p>

<p>Our task for this assignment is to create a behavioral prototype to explore the user interaction scenario of three choices: Speech to text dictation, Home alarm system, or Gesture recognition platform. After extensive research on all three topics, our group decided to work on speech-to-text dictation system, considering it as an inspiring and challenging topic.</p>

<p>The scenario for our testing is the user using speech to text dictation application that makes use of voice recognition to create and edit simple documents. This includes using spoken language to input text and using verbal command for simple text editing tasks.</p>

<p>We were particularly interested in exploring the following two questions through the user testing:</p>

<ul>
<li>Expectation of verbal commands for text editing tasks</li>
<li>Tolerance for errors during dictation.</li>
</ul>


<p>We expected to use low-cost, low-tech materials to build the prototype, in order to speed up our explore our testing and modification process, and also achieve the goal of answering user experience questions. The prototype should allow us to modify rapidly, which gives us the flexibility to control and compare the variable we want to observe. Last but not least, we would like to create a realistic interactive experience of our prototype, just like any user using a real dictation application during testing.</p>

<h2>Prototype</h2>

<p>While planning out our prototype, we came up with three approaches of building it:</p>

<h4>Google Drive</h4>

<p><img class="left" src="/images/prototyping/a4/prototype.png" width="300"></p>

<p>The first approach is using Google Drive (Google Doc) to type down the speech manually. The user will be asked to giving verbal commands to a laptop that shows a window of Google Doc, while a VoIP application running in the background that connects our wizard. Our wizard would be one of our teammate, acting as a part of our prototype: sitting somewhere else, using VoIP to hear the user’s voice, and typing down anything he heard on the document, which will also show up on the user’s screen.</p>

<p>Our concern of this prototype is that our typing speed is much slower than an actual dictation program. Plus the tooltips on the typing cursor showing the typist’s name, a user can easily tell it’s someone typing in the background.</p>

<h4>Simple Web Program</h4>

<p>The second approach is to build a simple web program, which allows you to prepare a paragraph of text, and pop up word by word in the testing. The tester will see the web program in an extended monitor with another operator in the other side, listening to Skype voice call, inserting words by clicking and modifying text manually.</p>

<p>The problem with this prototype is that we need to ask the user to input a specific paragraph of text. And when we are modifying the text, the user can see the mouse and cursor moving, which makes it seem less realistic.</p>

<h4>Google Slides</h4>

<p>The third idea is using Google Slides to pop up the pre-defined paragraph word by word while the user speaking. It’s very similar to the second approach while it also allow us to do more complicated modification during the testing. However, the drawback is that it’s hard to modify anything in the middle of the testing because you can’t just create new slides in front of the user, and if you change one of the slides, it will be discontinuoud to the next ones.</p>

<h4>Decision: Google Drive, &amp; Setup</h4>

<p><img class="right" src="/images/prototyping/a4/decision.jpg" width="300"></p>

<p>After trying out different approaches and comparing their performance, we decided to choose the Google Drive one, since it provides more flexibility for us than other designs to modify prototype in short time, which helps up explore the users’ expectation of verbal editing commands in both greater width and depth.</p>

<p>Part of our prototype was connectivity, and we applied FaceTime to do so. We made FaceTime run in the background to transfer user’s sound to our wizard, while our wizard typing down words manually on Google Doc from another laptop. We were hoping this would make the prototype look like a real system or provide environment similar to a real system.</p>

<p>We were attempting to convince our users that they are testing some part of a real dictating application. Below shows how we created the context:</p>

<ul>
<li>Our wizard and users were staying in different rooms, and the tester would not be (or be less) aware of typing sound by wizard.</li>
<li>Since we were using Google Drive as a main role of our prototype, we told our user that our system is an extension of Google Drive. We readjusted the display of Google Drive, and put on an annotation, “Google Drive Dictation System / Trial”, to make user ignore the low fidelity parts of prototype.</li>
<li>We typed down anything that the user said during the test, pretending to be a not-smart-enough system.</li>
<li>During the test, we arranged and applied several possible mistakes made by system, in order to observe user’s error handling.</li>
</ul>


<h2>Testing 1</h2>

<p><img class="left" src="/images/prototyping/a4/testing1.png" width="300"></p>

<p>Initially, we conducted an exploratory user test on two participants. We expected to explore users’ reaction to general functions and events of a speech-to-text system: structure of users’ verbal commands, error handling behavior during dictation, and complexity of their commands (system’s perspective). Therefore our prototype had no pre-defined set of verbal commands, which gave the users flexibility to use any verbal command of their choice.</p>

<p>We provided them with a short passage to dictate to the speech-to-text system. Users were also asked to perform the following tasks and to ensure that each sentence was correctly generated by the system.</p>

<h4>Passage</h4>

<p>The giant panda is perhaps the most powerful symbol in the world when it comes to species conservation. This peaceful, bamboo-eating member of the bear family faces a number of threats. It’s forest habitat is fragmented and populations are small and isolated from each other.</p>

<h4>Task</h4>

<p>All operations should be verbal commands. No other available input methods.
* Input the first sentence
* Input the second sentence
* Input the third sentence
* Replace the second sentence with Existing pandas are facing lots of threats.</p>

<p>The wizard typed down the sentences while users spoke. He/she would generate predefined mistakes in the sentences, which enabled users to provide verbal commands to correct the mistakes.</p>

<h2>Evaluation 1</h2>

<h4>Dictation/Error Recognition</h4>

<p><img class="right" src="/images/prototyping/a4/evaluation1.png" width="300"></p>

<p>Since the users read from a passage on a paper, they dictated at a quite fast pace and paid more attention to the text they were reading than the text being displayed on the screen. They only noticed a mistake made by the system after they were done reading a particular sentence. They reviewed the generated text in order to find mistakes and give commands to the system to correct them.</p>

<h4>Complexity of Verbal Commands</h4>

<p>One interesting finding was that the use of natural language in verbal commands. Some of their commands for editing was far from trying to match operations that are normally provided by keyboard or mouse. Instead of regularized/robotic commands, they operated in phrases and complete sentences rather one word.</p>

<p>Following are the examples of verbal commands they used in correcting errors made by the system:</p>

<ul>
<li>Change consolation to conservation.</li>
<li>No, the bamboo is not pregnant. it is fragmented.</li>
<li>Instead of stress put threats.</li>
<li>Replace stress with threat.</li>
<li>Correct et to eating.</li>
<li>Add a comma after peaceful.</li>
<li>Replace pregnant with fragmented.</li>
<li>Delete the second sentence.</li>
</ul>


<h4>Same Function, Different Commands</h4>

<p>Also, users used different commands for correcting same mistakes in a particular scenario. They expected the system to understand or accommodate multiple commands for the same function instead of using one specific command.</p>

<h4>Improvement in Next Iteration</h4>

<p>At this round we found users frequently questioning about the authenticity of the system, since the system only responded to the speech we assigned them. We couldn&rsquo;t evaluate user tolerance for error handling because the errors were few and the wizard responded to any command the user gave. Additionally, since the users were restricted to a set of instructions, the commands were only limited to changing a particular word or sentence.  The test was not sufficient enough to get meaningful answers to our questions. As a result, we refined our prototype and conducted further testing.</p>

<h2>Testing 2</h2>

<p><img class="left" src="/images/prototyping/a4/testing2.png" width="300"></p>

<p>We found two other users for our second round of testing. We provided them with the same passage and made a few changes to the task.</p>

<h4>Task</h4>

<ul>
<li>Input the first sentence</li>
<li>Input the second sentence</li>
<li>Input any sentence of your choice</li>
</ul>


<p>This time, the wizard responded to not only the input text dictated but every word the user said to make it seem more believable. Also, the wizard deliberately doesn&rsquo;t respond to some error correction commands made by the user to test what other ways the user might give commands for correcting the same mistake. We wanted to explore more commands a user would use and see if there were predictable patterns of commonly used commands.</p>

<h2>Evaluation 2</h2>

<p>With adding a random scenario and typing everything uttered, users were convinced that this was an authentic system.</p>

<h4>Dictation/ Error Recognition</h4>

<p>In this round of testing, we noticed that users paid more attention to the screen when dictating a random sentence to the computer and could easily identify errors made by the computer and make just-in-time corrections. Also, they dictated in a slower pace than they did when reading from a text.</p>

<h4>Similar commands</h4>

<p><img class="right" src="/images/prototyping/a4/evaluation2.png" width="300"></p>

<p>We also noticed similarity of words and phrases used as voice commands for editing and correcting an error across all users.
Commonly used phrases include:</p>

<ul>
<li>Change incorrect word to correct word</li>
<li>Replace incorrect word to correct word</li>
<li>Delete incorrect word and replace with correct word</li>
<li>Correct incorrect word with correct word</li>
</ul>


<h4>Error Handling / Correction</h4>

<p>When correction was done in real time, the user simply repeats the word until the system gets it right. If after multiple tries, the system doesn’t understand or respond to their command, they spell out the word to the system.</p>

<h4>Improvement in Next Iteration</h4>

<p>Although making changes made our system more authentic and enabled us to get more insights on similar commands, The results were not very different from the previous test and the commands were also still limited to the change and delete functions. We realized that giving the user predefined text to read from wasn&rsquo;t so effective and decided to perform another test without any predefined text or task.</p>

<h2>Testing 3</h2>

<p><img class="right" src="/images/prototyping/a4/testing3.png" width="300"></p>

<p>For the final round of testing, we simply asked our user to speak any sentences that came to mind gave them the freedom to do any editing task. Here, the wizard kept generating many random and repetitive errors in order to have a better understanding of how tolerant a user can be of errors generated by the system.</p>

<h2>Evaluation 3</h2>

<p>By putting the user in a more flexible scenario, we got more insights on verbal commands for other functions such as moving cursors, copy and paste and navigating to a particular line.</p>

<h4>Moving cursors</h4>

<p>User came up with the following commands to navigate the cursor to a desired location for editing.</p>

<ul>
<li>go up &ndash; To move cursor up</li>
<li>go down &ndash; To move cursor down</li>
<li>go back &ndash; To move cursor left</li>
<li>go to 4th line, delete this line</li>
</ul>


<h4>Cut and Paste</h4>

<p>Surprisingly, the user used the cut and paste function to move a particular sentence to the end of the document. At first the user gave a complex command:</p>

<blockquote><p>“Cut this line and paste it in the end of the document.”</p></blockquote>

<p>But when the user realized that the system was unresponsive to her command she split the sentence in two separate command and said each command one after the other.</p>

<h4>Error Handling</h4>

<p>Because the system kept generating random and repetitive errors,  the user kept repeating commands for a while before the system responded to the command which frustrated them.</p>

<h2>Conclusion</h2>

<p>From the testing, we can deduce that the set up of our prototype the use of google doc and facetime made it effective. Learning from each test and iterating and refining our design helped us to gain valuable knowledge about users expectations of verbal commands and their tolerance for errors in a simple dictation/editing task.</p>

<h4>Overall User Expectation of Verbal commands</h4>

<p>Users expected to use phrases and sentences in their natural language as verbal commands for editing, and they also expected the flexibility of saying a command in multiple ways so they don’t have to learn the command. Words such as “change”, “replace”, “correct” and “delete” were commonly used across all users when changing or correcting a word.</p>

<h4>Overall Tolerance for Error Handling</h4>

<p>Our testing showed that users were quite tolerant of occasional mistakes as they did not expect the system to perfectly get everything right. They only became frustrated when the system repeatedly failed to understand a particular utterance or their commands.</p>

<h2>Takeaway</h2>

<p>The fidelity of behavioral prototype does not necessarily influence the quality of testing. User can be way more engaged into the testing scenario than we expect, if with right instructions. The point is to approach the same goal of learning with minimal effort of building prototyping, which is especially perfect for rapid modification based on result and feedback of each testing.</p>

<p>Although the fidelity does not necessarily matter, a thorough plan is required for the entire process. We planned out the structure and general workflow for our prototyping and testing in advance, in order to make sure our process supported by strong base of theories and analysis. In this way, every possible modification during the iteration of prototyping and testing would all follow the major direction of our plan, which makes work faster, more precise, and easier to facilitate.</p>
</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-02-08T13:23:00-08:00" pubdate data-updated="true">Feb 8<span>th</span>, 2014</time></div>
	<div class="tags">


	<a class='category' href='/blog/categories/hcid/'>HCID</a>, <a class='category' href='/blog/categories/prototyping-studio/'>Prototyping Studio</a>


</div>
	
	<div class="comments"><a href="#disqus_thread">Comments</a></div>
	
</div>
</article>

	<div class="share">
	<div class="addthis_toolbox addthis_default_style ">
		
		
		<a class="addthis_button_tweet"></a>
		
		
		
	</div>
	
</div>



<section id="comment">
    <h2 class="title">Comments</h2>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</section>
</div>
	<footer id="footer" class="inner">Copyright &copy; 2014

    Chaoyu Yang

</footer>
	<script src="/javascripts/slash.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script> <!-- Delete or comment this line to disable Fancybox -->


<script type="text/javascript">
      var disqus_shortname = 'hcidus';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://hcid.us/blog/2014/02/08/behavior-prototype-assignment-speech-to-text-dictation-system/';
        var disqus_url = 'http://hcid.us/blog/2014/02/08/behavior-prototype-assignment-speech-to-text-dictation-system/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//go.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-45122468-1']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>



</body>
</html>
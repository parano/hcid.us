<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: HCID | Adventures in HCI+D]]></title>
  <link href="http://hcid.us/blog/categories/hcid/atom.xml" rel="self"/>
  <link href="http://hcid.us/"/>
  <updated>2014-03-06T21:10:33-08:00</updated>
  <id>http://hcid.us/</id>
  <author>
    <name><![CDATA[Chaoyu Yang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Electronics Prototype: Pomoduino Timer]]></title>
    <link href="http://hcid.us/blog/2014/03/03/electronics-prototype-pomoduino-timer/"/>
    <updated>2014-03-03T14:10:25-08:00</updated>
    <id>http://hcid.us/blog/2014/03/03/electronics-prototype-pomoduino-timer</id>
    <content type="html"><![CDATA[<h2>Background</h2>

<p><img class="right" src="/images/prototyping/a7/clock.jpg" width="230"></p>

<p>Building interactive physical systems is a great joy to me. It’s always fun to design the circuits, solder different parts and make things work. This week, our prototyping studio class gives me a chance to build an electronics prototype with Arduino.</p>

<p>As there is only one week for me to do this assignment, I decide to make something useful to me and also small enough so that I can handle it in one week. Looking at my dumped alarm clock, I decide to build a Pomodoro Timer out of it.</p>

<p><a href="http://en.wikipedia.org/wiki/Pomodoro_Technique">Pomodoro Technique</a> is a popular time management method. Based on the idea that frequent breaks can improves mental agility, this method breaks down the work into intervals, 25 minutes in length, and separated by short breaks.</p>

<p>To apply Pomodoro Technique in real life, people invented the Pomodoro Timer which is similar to a kitchen timer, but can only time 25 minutes every time. The physical act of winding up the timer confirms the user’s determination to start a task, the ticking to externalises desire to complete the task, and the ringing announces a break.</p>

<p>For this prototyping exercise, I made a pomodoro timer mainly with an Arduino UNO board, a LCD screen, and a motor.  The LCD screen is used to visualize the ticking and how much time left in the running pomodori. The button is the only way to interact with the system which can toggle the status between running and reseted.</p>

<!-- more -->


<p>Here is a demo video of the pomodoro timer I made, which I call it <em>pomoduino</em>:</p>

<div class="video-container">
<iframe src="http://hcid.us//player.vimeo.com/video/88049469" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
</div>




<hr>


<h2>Playing With Electronics And Arduino</h2>

<h4>Motor Module</h4>

<p><img src="/images/prototyping/a7/motor1.jpg" width="400"><img src="/images/prototyping/a7/motor2.jpg" width="400"></p>

<p>The motor module is used to power the bell in the alarm clock. I firstly prepare and test the circuits with an extra motor and migrate it to the motor inside the clock in the end.</p>

<p>The transistor acts like a switch to control the motor; it make use of the signal from Arduino to power another circuit which much higher current. The diode and capacitor in this circuit are meant to protect the circuit from noise and spikes.</p>

<h4>LCD Screen Module</h4>

<p><img src="/images/prototyping/a7/screen1.jpg" width="400"><img src="/images/prototyping/a7/screen2.jpg" width="400"></p>

<p>The 16 pins LCD screen is widely used with single-chip microcomputers. With arduino, the library and driver for controlling this LCD is actually pre-installed in the Arduino IDE. The challenge here is to neatly layout all these wires and present a clear interface to the user. I bundled all the wires to go through the hole in the center of the clockface, and stick the small breadboard on the surface.</p>

<h4>Button Module</h4>

<p><img class="right" src="/images/prototyping/a7/button.jpg" width="300"></p>

<p>The button here actually works like a switch that can change the state of the system. The system have to keep track on the current and previous value of the button, and only change the state of the system when its value change from LOW to HIGH. That means the state of the system will only be changed when the user push down the button. A tutorial on this could be found <a href="http://www.arduino.cc/en/Tutorial/Switch#.UxegtPRdXHA">here</a>.</p>

<h4>Putting Everything Together</h4>

<p>After testing each part, I start to assemble different parts of the hardware together. This is the overall layout of the circuits:</p>

<p><img src="/images/prototyping/a7/circuits.jpg"></p>

<p>I apply a 9 volts battery to power both the arduino and the motor. The input-output interface(button, LCD screen and a breadboard) is separate from the others, and is positioned in the front of the ‘clock’. Another breadboard is attached to the arduino board, together in the back of the ‘clock’, and used to arrange the motor circuits. I removed the original wires that connected to the motor inside the clock, and soldered another two that is easier to connect to the breadboard.</p>

<p><img src="/images/prototyping/a7/alltogether.jpg" width="400"><img src="/images/prototyping/a7/back.jpg" width="400"></p>

<h2>Reflections</h2>

<h4>Constraints Of Prototyping</h4>

<p>During prototyping electronics, the components we have are usually not ideally to our design. In this project for example, I want to present ‘push button to start’ on the LCD screen. But the LCD is not long enough to contain this sentence so that I change it to a less appropriate one: ‘click to start’.</p>

<p>I recognized that there are always lot of constraints in prototyping electronics, because we can’t get all the hardware parts customized to our need. But most of them it doesn’t matter to the prototype itself, as long as we can continue the following usability testing upon the prototype. Some walkaround could help to build the initial prototype.</p>

<h4>Software Engineering Principles</h4>

<p>Building and debugging hardware seems very challenge to me in the beginning. Until I find that I can actually apply my knowledge in developing software to building electronics. The principle of Modular Programming, Encapsulation and API design are also quite useful in building hardware. For example, if I haven’t divided the circuits into separate modules in the beginning, it would take much more effort to deal with all those wires, and also much harder to debug potential issues in hardware or software.</p>

<h4>Be Safe &amp; Follow Best Practice</h4>

<p>Playing with electronics is not safe especially when high current circuits are involved in your project. Even with only 9 or 5 volts battery, it’s really easy to break some components in the circuits. We can’t expect every prototyper to be an expert in digital design and create appropriate protect circuits. But as a prototyper, it is important to follow the best practice from all those tutorials and examples, so as to avoid those dangers.</p>

<h4>Fast Prototyping with Arduino</h4>

<p>Building prototypes with Arduino is so fast. There is a huge and active online community that contributes to millions of libraries and tutorials. You can easily get start with any type of sensor or components, by just following the tutorials, and use related libraries. The processing language also allow the user to integrate their application with other programming language or applications. I think this is the reason that Arduino is a good choice for prototyping electronics. The less effort you spend on technical problems, the more time you could focus on solving the real problem.</p>

<p><img src="/images/prototyping/a7/books.jpg" title="center" ></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mobile Prototype: Tweak The Tweet]]></title>
    <link href="http://hcid.us/blog/2014/02/28/mobile-prototype-tweak-the-tweet/"/>
    <updated>2014-02-28T11:30:05-08:00</updated>
    <id>http://hcid.us/blog/2014/02/28/mobile-prototype-tweak-the-tweet</id>
    <content type="html"><![CDATA[<h2>Background</h2>

<p><img class="right" src="/images/prototyping/a6/designspecification.jpg" width="230"></p>

<p>This week, our HCID 520 prototyping class is ready to extend our prototyping skills to building mobile applications. Our task is to build a working prototype of the TtT(<a href="http://faculty.washington.edu/kstarbi/tweak-the-tweet.html">Tweak The Tweet</a>) mobile app, to support future usability study.</p>

<p>TtT is an application that is part of HCDE Professor <a href="http://faculty.washington.edu/kstarbi/">Kate Starbird</a>’s work in crisis research. She studies the “use of social media during crisis events, specifically looking at how the converging audience (aka, the ‘crowd’) can contribute—and is already contributing—to crisis response efforts.“</p>

<p>TtT uses Twitter to gather and direct information during crises to people who can act on it to the benefit of affected people and communities. And TtT aims to allow digital volunteers to provide information in a form that is more easily and reliably processed and analyzed.</p>

<p>We are given the design specification of this application that written by Grace Jang. It includes all the user interfaces, interactions and functional requirements of this app. And the prototype is suppose to be consistent with the specification.</p>

<!-- more -->


<p>Here is a short video demo about this mobile prototype I developed:</p>

<div class="video-container">
<iframe src="http://hcid.us//player.vimeo.com/video/87334846" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
</div>


<h2>Prototyping</h2>

<p><img class="left" src="/images/prototyping/a6/phonegap.jpg" width="270"></p>

<p>There are lots of tools for prototyping high fidelity mobile applications. Considering the scale of this application, non-programming approaches, such as App Inventor or Axure, might be very hard to manage and even harder to adapt to change in future works. So I choose PhoneGap(also known as Apache Cordova) and web technology to build this prototype since I am already very familiar with web programming.</p>

<p>PhoneGap is a mobile development framework that allows software engineers to build application for mobile devices using web technologies(JavaScript, HTML5, CSS3). It use a middle layer that contains a webview inside a native app to render the UIs, and integrate many system service and apis into corresponding JavaScript functions.</p>

<p>Taking advantage of using web, I can create highly customized user interface using HTML and CSS. And thanks to jQuery Mobile and Bootstrap, creating the UIs defined in the specification is not hard at all. Here is some screenshot of the UIs I made using web:</p>

<p><img src="/images/prototyping/a6/1.jpg" width="266"><img src="/images/prototyping/a6/2.jpg" width="266"><img src="/images/prototyping/a6/3.jpg" width="266"></p>

<p><img src="/images/prototyping/a6/4.jpg" width="266"><img src="/images/prototyping/a6/5.jpg" width="266"><img src="/images/prototyping/a6/6.jpg" width="266"></p>

<p><img src="/images/prototyping/a6/7.jpg" width="266"><img src="/images/prototyping/a6/8.jpg" width="266"><img src="/images/prototyping/a6/9.jpg" width="266"></p>

<p>I started with build a mobile web application: using a local HTTP server to host the application and do testing in the browser. PhoneGap also provides a toolkit for developers to create all the files you need to pack up the web app into a native app installer. This allow me to use XCode to load the project folder generated from PhoneGap, and install this application on an iOS emulator.</p>

<p><img src="/images/prototyping/a6/vim.jpg" width="400"><img src="/images/prototyping/a6/xcode.jpg" width="400"></p>

<h2>Reflection</h2>

<p><img class="right" src="/images/prototyping/a6/login.jpg" width="230"></p>

<p>One thing confused me a lot in the beginning is the authentication page in the design specification. It defined a customized login page for twitter and suppose that page can be implemented. But in fact, this kind of customized login page only exist before Twitter canceled their support for <code>HTTP Basic Auth</code> due to security reason. Nowadays twitter only support <code>OAuth</code> for authentication, which means the mobile app will redirect to a web page provided by twitter and not allowed to store the users’ password.</p>

<p>It’s possible that the author of this design specification didn’t discuss its design with a developer in detail. So that he or she can not really recognize all the constraints in designing this application. Moreover, since this app only need twitter authentication to post a tweet, it’s more appropriate to use <a href="https://dev.twitter.com/docs/ios/using-tweet-sheet">Twitter Sheet</a> or for PhoneGap, the <a href="https://dev.twitter.com/docs/intents">Web Intents</a>. Because in that case, the application itself don’t need to worry about the authentication.</p>

<p>In general, I think prototyping TtT app using the web is a good choice. Comparing to developing native application, developing a mobile web application is arguably cheaper and faster, which is exactly what we expect from a prototyping tool. Although there are some constraints of building mobile app using web, including offline functioning, system level features, and UI responding speed, but the TtT app doesn’t require any offline feature or system level apis. Also for prototyping, the speed of webview is totally acceptable.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Website Prototype: dub Site Redesign]]></title>
    <link href="http://hcid.us/blog/2014/02/15/website-prototype-assignment-dub-site-redesign/"/>
    <updated>2014-02-15T16:57:15-08:00</updated>
    <id>http://hcid.us/blog/2014/02/15/website-prototype-assignment-dub-site-redesign</id>
    <content type="html"><![CDATA[<h2>Background</h2>

<p>Our weekly prototyping assignments started with low fidelity prototyping to higher fidelity ones. This week we are proposed to build an interactive wireframe for a website prototype. We are giving an existing website(our very own dub website), to analyze its current content and functionality and redesign the information architecture, navigation and general layout.</p>

<h2>Design</h2>

<p><img class="right" src="/images/prototyping/a5/dub-site.png" width="300"></p>

<p>After an extensive research and analysis on the current dub website (dub.washington.edu/), I got a basic understanding of all the functionality and content of this site. And synthesis with three user interviews about their experience of using this website, I have the following findings about users’ usual practice of using this site:</p>

<ul>
<li>Calendar is the most regularly used functionality for dub members, to track the newest events, specially the weekly dub seminar;</li>
<li>For other audience who want to get to know about dub community, the people and their publications/projects are the most visited pages;</li>
<li>The blog is less popular among visitors which may because of its lack of updates;</li>
</ul>


<p>According to all these findings, I think there are several main problems in terms of information architecture:</p>

<!-- more -->


<ul>
<li>The most regularly used information(events, seminars) is located in secondary pages instead of homepage, and the upcoming event is not being highlighted.</li>
<li>There are some huge list of names/titles of people/publication/project which makes it difficult to navigate through. A better index or search function is needed.</li>
<li>The connections between dub people, and their publications and projects are not effectively presented.</li>
</ul>


<h2>Prototyping Process</h2>

<p>To address these problems, I redesigned the information architecture of this site while remaining most of the functionalities of the original website. Here I will introduce my new design:</p>

<h3>Home Page</h3>

<p>I use the masthead area to present the most important news and announcements. I envision this is a dynamic area with a wide image as background, so as to give new visitors a visualized impression of dub.</p>

<p>Following the masthead, is a selected partition of the most recent blog posts and news with some thumbnails and a detailed text paragraph of introduction with a link to view the whole article. Here I wish to present all the must-know news, announcements as well as the upcoming dub seminar, so that dub members could get to know about the weekly talk directly from the home page.</p>

<div class="center">
<img src="/images/prototyping/a5/index.png" width="380"><img src="/images/prototyping/a5/drop-down-menu.png" width="380">
</div>


<p>The DUB logo on the top left corner is the top level menu across the whole site in the format of a hover to drop-down menu. User can use this menu as a shortcut to quickly redirect to the target resource.</p>

<h3>People/Publications/Projects Page</h3>

<p>The DUB people page lists all the dub members and provides a link to their own page. The current website has a huge list of names with only their department. It’s actually very annoying for new visitors to get to know about the people here. Take my experience for example, to find a professor interested in the field of data visualization, I have to press ‘command’ key and click on all the links, and then check them one by one.</p>

<p>It will be much better if we can show some rudimentary information about the professor in the listing page. But the drawback of this approach is you need to scroll down the page a lot to find a specific one.</p>

<p>So in my design, I try to combine both approaches where I put the name list on the left as a quick reference. And put a list of short introductions with a portrait image on the right side. In order to navigate to a certain people faster, I made the reference list a folding menu list because it’s actually a very long list referring to the current website.</p>

<p><img class="center" src="/images/prototyping/a5/people-list.png" width="500"></p>

<p>For the current dub site, I found these are a lot of links between the dub people, the projects and the publications. But by click on a link and jump to another page, users can’t really percept or memorize all the connections easily. They are connected but visually disconnected.</p>

<p>Take into account this observation, I designed this vertical accordion page to show each person’s related publications and projects. There are three columns in this accordion page where the introduction page is unfolded by default. When the user click on the publications or the projects, the corresponding partition will unfold and the previously unfold column will be enfolded:</p>

<p><img class="center" src="/images/prototyping/a5/accordion.gif" width="500"></p>

<p><img src="/images/prototyping/a5/people-name.png" width="266"><img src="/images/prototyping/a5/people-publications.png" width="266"><img src="/images/prototyping/a5/people-projects.png" width="267"></p>

<h3>Calendar Page</h3>

<p>I personally don’t like using such calendar view to show recent events, but most dub members are already getting used to this approach and the current calendar page is actually the most visited page according to the survey. So I keep the same functionality in the new design, and put the entrance on the upper right corner of the navbar for faster accessing.</p>

<h2>Prototyping Tools</h2>

<p>To create such an interactive wireframe prototype, I have several different tool to choose from: using HTML/CSS or using a specification prototyping software such as Axure, Balsamiq etc. As I’m already very familiar with developing websites, I choose to learn and use Axure this time so as to learn something new.</p>

<p>Building the home page in Axure is pretty easy. I made it without reading any extra documentation or tutorial. Because most elements in the homepage are very basic, and the interaction with those elements are very simple. But when I started to build the people profile page, it becomes very tricky. I searched for several implementation of accordion menu in Axure, but they are neither too expensive, nor have no documentation or any clue for you to reuse it. To work around this problem, I build three different pages and use link to simulate the accordion page, but the transition is too slow.</p>

<p><img class="right" src="/images/prototyping/a5/axure.png" width="400"></p>

<p>Another thing about axure is that comparing to using template in web development, it’s really hard to reuse partition of the page in other pages. I have to copy the navbar, the footer, and the navigation menu on every page. And each time I want to update a link in the menu, I have to change it on every page, which is very annoying.</p>

<p>After creating this prototype in Axure, I guess I will not use Axure again. My reason is that comparing using HTML/CSS/Javascript to using Axure, the only drawback is the learning curve is relatively higher. However, for someone who is already familiar with web developing, there is no need to learn Axure. Actually with the help of countless open source components and frameworks(Bootstrap, Blueprint) from the web development community, a developer can build the prototype very quickly.</p>

<p>Another reason is that for only creating the wireframe, using Axure may be easier. Nevertheless, to learn to implement some advanced interaction in Axure, it may take even more time than the HTML/CSS/Javascript approach.</p>

<p>Thanks to Axure, I can publish the prototype online: <a href="http://fik3lo.axshare.com/.">http://fik3lo.axshare.com/.</a> Not all the links in this page can work due to time constraint.</p>

<h2>Reflection</h2>

<p>Here is some feedbacks I collected from other classmates:</p>

<ul>
<li><p>Drop down menu on the top left corner may need some improvement. There is plenty space on the top of the page for all the links, replace it using a navigation bar with link to ‘home’, ‘people’, ‘publications’, will be better.</p></li>
<li><p>No one knows how to use the Search on the top right corner. I think it’s better to add a tooltips for the search functionality.</p></li>
<li><p>Didn’t consider the situation where the faculty or student don’t have any publications/projects. The page will definitely change a lot or have some blank.</p></li>
</ul>


<p>I also created a video to introduce my design via recording people using this prototype. Most of the interaction in the video in understandable to audiences, and they can get the idea of showing the connections between people and their publications/projects through a single page accordion menu. Nevertheless, my tester got confused when he was using the prototype alone. The page redirection transition makes it harder to perceive this idea.</p>

<p>Here is the video I made to present this website prototyp:</p>

<div class="video-container">
<iframe src="http://hcid.us//player.vimeo.com/video/86963982" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
</div>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Behavior Prototype: Speech-to-Text Dictation System]]></title>
    <link href="http://hcid.us/blog/2014/02/08/behavior-prototype-assignment-speech-to-text-dictation-system/"/>
    <updated>2014-02-08T13:23:00-08:00</updated>
    <id>http://hcid.us/blog/2014/02/08/behavior-prototype-assignment-speech-to-text-dictation-system</id>
    <content type="html"><![CDATA[<blockquote><p>This article and its related work is developed by Hadiza Ismaila, Chaoyu Yang, and Chase Chen-Hung Wu.</p></blockquote>

<p>In order to learn about user’s behavior of intuitive verbal commands on speech-to-text system, we designed and implemented a rapid behavioral prototype for user testing. Built by Google Drive with a person playing the speech-to-text system (a.k.a. Wizard of Oz), our rapid prototype was tested and modified for several rounds with approaches such as simple text input, editing, error handling, and cursor moving. We summarized and organized the result into different categories of interactive patterns, and came up with valuable learnings and insights from our result analysis.</p>

<div class="video-container">
<iframe width="640" height="360" src="http://hcid.us//www.youtube.com/embed/p3B-Bi3ZO8g" frameborder="0" allowfullscreen></iframe>
</div>




<!-- more -->


<h2>Background</h2>

<p><img class="right" src="/images/prototyping/a4/draft.png" width="250"></p>

<p>Our task for this assignment is to create a behavioral prototype to explore the user interaction scenario of three choices: Speech to text dictation, Home alarm system, or Gesture recognition platform. After extensive research on all three topics, our group decided to work on speech-to-text dictation system, considering it as an inspiring and challenging topic.</p>

<p>The scenario for our testing is the user using speech to text dictation application that makes use of voice recognition to create and edit simple documents. This includes using spoken language to input text and using verbal command for simple text editing tasks.</p>

<p>We were particularly interested in exploring the following two questions through the user testing:</p>

<ul>
<li>Expectation of verbal commands for text editing tasks</li>
<li>Tolerance for errors during dictation.</li>
</ul>


<p>We expected to use low-cost, low-tech materials to build the prototype, in order to speed up our explore our testing and modification process, and also achieve the goal of answering user experience questions. The prototype should allow us to modify rapidly, which gives us the flexibility to control and compare the variable we want to observe. Last but not least, we would like to create a realistic interactive experience of our prototype, just like any user using a real dictation application during testing.</p>

<h2>Prototype</h2>

<p>While planning out our prototype, we came up with three approaches of building it:</p>

<h4>Google Drive</h4>

<p><img class="left" src="/images/prototyping/a4/prototype.png" width="300"></p>

<p>The first approach is using Google Drive (Google Doc) to type down the speech manually. The user will be asked to giving verbal commands to a laptop that shows a window of Google Doc, while a VoIP application running in the background that connects our wizard. Our wizard would be one of our teammate, acting as a part of our prototype: sitting somewhere else, using VoIP to hear the user’s voice, and typing down anything he heard on the document, which will also show up on the user’s screen.</p>

<p>Our concern of this prototype is that our typing speed is much slower than an actual dictation program. Plus the tooltips on the typing cursor showing the typist’s name, a user can easily tell it’s someone typing in the background.</p>

<h4>Simple Web Program</h4>

<p>The second approach is to build a simple web program, which allows you to prepare a paragraph of text, and pop up word by word in the testing. The tester will see the web program in an extended monitor with another operator in the other side, listening to Skype voice call, inserting words by clicking and modifying text manually.</p>

<p>The problem with this prototype is that we need to ask the user to input a specific paragraph of text. And when we are modifying the text, the user can see the mouse and cursor moving, which makes it seem less realistic.</p>

<h4>Google Slides</h4>

<p>The third idea is using Google Slides to pop up the pre-defined paragraph word by word while the user speaking. It’s very similar to the second approach while it also allow us to do more complicated modification during the testing. However, the drawback is that it’s hard to modify anything in the middle of the testing because you can’t just create new slides in front of the user, and if you change one of the slides, it will be discontinuoud to the next ones.</p>

<h4>Decision: Google Drive, &amp; Setup</h4>

<p><img class="right" src="/images/prototyping/a4/decision.jpg" width="300"></p>

<p>After trying out different approaches and comparing their performance, we decided to choose the Google Drive one, since it provides more flexibility for us than other designs to modify prototype in short time, which helps up explore the users’ expectation of verbal editing commands in both greater width and depth.</p>

<p>Part of our prototype was connectivity, and we applied FaceTime to do so. We made FaceTime run in the background to transfer user’s sound to our wizard, while our wizard typing down words manually on Google Doc from another laptop. We were hoping this would make the prototype look like a real system or provide environment similar to a real system.</p>

<p>We were attempting to convince our users that they are testing some part of a real dictating application. Below shows how we created the context:</p>

<ul>
<li>Our wizard and users were staying in different rooms, and the tester would not be (or be less) aware of typing sound by wizard.</li>
<li>Since we were using Google Drive as a main role of our prototype, we told our user that our system is an extension of Google Drive. We readjusted the display of Google Drive, and put on an annotation, “Google Drive Dictation System / Trial”, to make user ignore the low fidelity parts of prototype.</li>
<li>We typed down anything that the user said during the test, pretending to be a not-smart-enough system.</li>
<li>During the test, we arranged and applied several possible mistakes made by system, in order to observe user’s error handling.</li>
</ul>


<h2>Testing 1</h2>

<p><img class="left" src="/images/prototyping/a4/testing1.png" width="300"></p>

<p>Initially, we conducted an exploratory user test on two participants. We expected to explore users’ reaction to general functions and events of a speech-to-text system: structure of users’ verbal commands, error handling behavior during dictation, and complexity of their commands (system’s perspective). Therefore our prototype had no pre-defined set of verbal commands, which gave the users flexibility to use any verbal command of their choice.</p>

<p>We provided them with a short passage to dictate to the speech-to-text system. Users were also asked to perform the following tasks and to ensure that each sentence was correctly generated by the system.</p>

<h4>Passage</h4>

<p>The giant panda is perhaps the most powerful symbol in the world when it comes to species conservation. This peaceful, bamboo-eating member of the bear family faces a number of threats. It’s forest habitat is fragmented and populations are small and isolated from each other.</p>

<h4>Task</h4>

<p>All operations should be verbal commands. No other available input methods.
* Input the first sentence
* Input the second sentence
* Input the third sentence
* Replace the second sentence with Existing pandas are facing lots of threats.</p>

<p>The wizard typed down the sentences while users spoke. He/she would generate predefined mistakes in the sentences, which enabled users to provide verbal commands to correct the mistakes.</p>

<h2>Evaluation 1</h2>

<h4>Dictation/Error Recognition</h4>

<p><img class="right" src="/images/prototyping/a4/evaluation1.png" width="300"></p>

<p>Since the users read from a passage on a paper, they dictated at a quite fast pace and paid more attention to the text they were reading than the text being displayed on the screen. They only noticed a mistake made by the system after they were done reading a particular sentence. They reviewed the generated text in order to find mistakes and give commands to the system to correct them.</p>

<h4>Complexity of Verbal Commands</h4>

<p>One interesting finding was that the use of natural language in verbal commands. Some of their commands for editing was far from trying to match operations that are normally provided by keyboard or mouse. Instead of regularized/robotic commands, they operated in phrases and complete sentences rather one word.</p>

<p>Following are the examples of verbal commands they used in correcting errors made by the system:</p>

<ul>
<li>Change consolation to conservation.</li>
<li>No, the bamboo is not pregnant. it is fragmented.</li>
<li>Instead of stress put threats.</li>
<li>Replace stress with threat.</li>
<li>Correct et to eating.</li>
<li>Add a comma after peaceful.</li>
<li>Replace pregnant with fragmented.</li>
<li>Delete the second sentence.</li>
</ul>


<h4>Same Function, Different Commands</h4>

<p>Also, users used different commands for correcting same mistakes in a particular scenario. They expected the system to understand or accommodate multiple commands for the same function instead of using one specific command.</p>

<h4>Improvement in Next Iteration</h4>

<p>At this round we found users frequently questioning about the authenticity of the system, since the system only responded to the speech we assigned them. We couldn&rsquo;t evaluate user tolerance for error handling because the errors were few and the wizard responded to any command the user gave. Additionally, since the users were restricted to a set of instructions, the commands were only limited to changing a particular word or sentence.  The test was not sufficient enough to get meaningful answers to our questions. As a result, we refined our prototype and conducted further testing.</p>

<h2>Testing 2</h2>

<p><img class="left" src="/images/prototyping/a4/testing2.png" width="300"></p>

<p>We found two other users for our second round of testing. We provided them with the same passage and made a few changes to the task.</p>

<h4>Task</h4>

<ul>
<li>Input the first sentence</li>
<li>Input the second sentence</li>
<li>Input any sentence of your choice</li>
</ul>


<p>This time, the wizard responded to not only the input text dictated but every word the user said to make it seem more believable. Also, the wizard deliberately doesn&rsquo;t respond to some error correction commands made by the user to test what other ways the user might give commands for correcting the same mistake. We wanted to explore more commands a user would use and see if there were predictable patterns of commonly used commands.</p>

<h2>Evaluation 2</h2>

<p>With adding a random scenario and typing everything uttered, users were convinced that this was an authentic system.</p>

<h4>Dictation/ Error Recognition</h4>

<p>In this round of testing, we noticed that users paid more attention to the screen when dictating a random sentence to the computer and could easily identify errors made by the computer and make just-in-time corrections. Also, they dictated in a slower pace than they did when reading from a text.</p>

<h4>Similar commands</h4>

<p><img class="right" src="/images/prototyping/a4/evaluation2.png" width="300"></p>

<p>We also noticed similarity of words and phrases used as voice commands for editing and correcting an error across all users.
Commonly used phrases include:</p>

<ul>
<li>Change incorrect word to correct word</li>
<li>Replace incorrect word to correct word</li>
<li>Delete incorrect word and replace with correct word</li>
<li>Correct incorrect word with correct word</li>
</ul>


<h4>Error Handling / Correction</h4>

<p>When correction was done in real time, the user simply repeats the word until the system gets it right. If after multiple tries, the system doesn’t understand or respond to their command, they spell out the word to the system.</p>

<h4>Improvement in Next Iteration</h4>

<p>Although making changes made our system more authentic and enabled us to get more insights on similar commands, The results were not very different from the previous test and the commands were also still limited to the change and delete functions. We realized that giving the user predefined text to read from wasn&rsquo;t so effective and decided to perform another test without any predefined text or task.</p>

<h2>Testing 3</h2>

<p><img class="right" src="/images/prototyping/a4/testing3.png" width="300"></p>

<p>For the final round of testing, we simply asked our user to speak any sentences that came to mind gave them the freedom to do any editing task. Here, the wizard kept generating many random and repetitive errors in order to have a better understanding of how tolerant a user can be of errors generated by the system.</p>

<h2>Evaluation 3</h2>

<p>By putting the user in a more flexible scenario, we got more insights on verbal commands for other functions such as moving cursors, copy and paste and navigating to a particular line.</p>

<h4>Moving cursors</h4>

<p>User came up with the following commands to navigate the cursor to a desired location for editing.</p>

<ul>
<li>go up &ndash; To move cursor up</li>
<li>go down &ndash; To move cursor down</li>
<li>go back &ndash; To move cursor left</li>
<li>go to 4th line, delete this line</li>
</ul>


<h4>Cut and Paste</h4>

<p>Surprisingly, the user used the cut and paste function to move a particular sentence to the end of the document. At first the user gave a complex command:</p>

<blockquote><p>“Cut this line and paste it in the end of the document.”</p></blockquote>

<p>But when the user realized that the system was unresponsive to her command she split the sentence in two separate command and said each command one after the other.</p>

<h4>Error Handling</h4>

<p>Because the system kept generating random and repetitive errors,  the user kept repeating commands for a while before the system responded to the command which frustrated them.</p>

<h2>Conclusion</h2>

<p>From the testing, we can deduce that the set up of our prototype the use of google doc and facetime made it effective. Learning from each test and iterating and refining our design helped us to gain valuable knowledge about users expectations of verbal commands and their tolerance for errors in a simple dictation/editing task.</p>

<h4>Overall User Expectation of Verbal commands</h4>

<p>Users expected to use phrases and sentences in their natural language as verbal commands for editing, and they also expected the flexibility of saying a command in multiple ways so they don’t have to learn the command. Words such as “change”, “replace”, “correct” and “delete” were commonly used across all users when changing or correcting a word.</p>

<h4>Overall Tolerance for Error Handling</h4>

<p>Our testing showed that users were quite tolerant of occasional mistakes as they did not expect the system to perfectly get everything right. They only became frustrated when the system repeatedly failed to understand a particular utterance or their commands.</p>

<h2>Takeaway</h2>

<p>The fidelity of behavioral prototype does not necessarily influence the quality of testing. User can be way more engaged into the testing scenario than we expect, if with right instructions. The point is to approach the same goal of learning with minimal effort of building prototyping, which is especially perfect for rapid modification based on result and feedback of each testing.</p>

<p>Although the fidelity does not necessarily matter, a thorough plan is required for the entire process. We planned out the structure and general workflow for our prototyping and testing in advance, in order to make sure our process supported by strong base of theories and analysis. In this way, every possible modification during the iteration of prototyping and testing would all follow the major direction of our plan, which makes work faster, more precise, and easier to facilitate.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Video Prototype: Introducing Supernome App]]></title>
    <link href="http://hcid.us/blog/2014/02/01/video-prototype-assignment-introducing-supernome-app/"/>
    <updated>2014-02-01T22:52:58-08:00</updated>
    <id>http://hcid.us/blog/2014/02/01/video-prototype-assignment-introducing-supernome-app</id>
    <content type="html"><![CDATA[<div class="video-container">
<iframe src="http://hcid.us//player.vimeo.com/video/85534963" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
</div>



]]></content>
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Adventures in HCI+D]]></title>
  <link href="http://hcid.us/atom.xml" rel="self"/>
  <link href="http://hcid.us/"/>
  <updated>2014-03-25T05:41:13-07:00</updated>
  <id>http://hcid.us/</id>
  <author>
    <name><![CDATA[Chaoyu Yang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[NecX: Wearable Game Controller for Neck Exercising]]></title>
    <link href="http://hcid.us/blog/2014/03/19/necx/"/>
    <updated>2014-03-19T13:09:12-07:00</updated>
    <id>http://hcid.us/blog/2014/03/19/necx</id>
    <content type="html"><![CDATA[<blockquote><p>This article and its related work is developed by <a href="http://aniket.io">Aniket Handa</a>, and Chaoyu Yang.</p></blockquote>

<h2>Summary</h2>

<p><img class="right" src="http://hcid.us/images/prototyping/fp/cover.jpg" width="240"></p>

<p>About a third of U.S. adults deal with neck and back problems. Here we seek to design a solution, which makes neck exercising fun. For this we use two prototyping techniques to quickly answer some questions regarding the design of the solution. We then user test the solution with two participants. The testing gave us interesting insights to “what works?” and “what might not work?” This helped us to scope down the problem and design a simple game-controlling interface that can be used with variety of games. Finally, the report outlines our analysis of the whole process.</p>

<h2>Problem</h2>

<p>According to recent survey, around 31% of U.S. adults have some sort of neck or back condition that causes them pain.  This condition gets worst with age.  Neck and back problems are highly correlated. Exercising the neck need more repetitions, and can easily become boring, the major reason it is neglected by majority of people. It also depends highly on lifestyle. The cubicle job has only promoted such problems. It is fascinating that there is no game targeted towards making exercise neck fun (As per our brief survey), not even using Microsoft Kinect. Here we present our problem questions, which are prototypes were suppose to answer.</p>

<h2>Design Question</h2>

<p><em>“How can we provide an environment which makes neck exercising fun?</em></p>

<p>Specific Questions</p>

<ul>
<li><p>Behavioural prototype:</p>

<ul>
<li><em>“What is the best way to interact with this type of system?”</em></li>
<li><em>“What level of accuracy will be the base line?”</em></li>
</ul>
</li>
<li><p>Hardware Prototype:</p>

<ul>
<li><em>“Is it technically feasible?&ldquo;</em></li>
<li><em>&ldquo; How will be the final experience like?”</em></li>
</ul>
</li>
</ul>


<h2>Prototypes</h2>

<h3>Behaviour Prototype</h3>

<p>In order to learn about user’s behavior of intuitive commands via neck movement, we created a rapid behavior prototype for user testing. We first listed down the neck exercises that we need the user to perform. The picture below shows some neck movements which are good for exercising.</p>

<p><img src="http://hcid.us/images/prototyping/fp/exercise.png">
<em>Neck Movements. From left to right: Users Right, Down, Up, Users Left, Spin.</em></p>

<!-- more -->


<p>Next step was to map these basic exercises, or one might say neck gestures, to actions/tasks which are needed to interact with an immersive game. We mapped these to Top, Down, Right, Left ( As in to replace Arrow Keys in a typical keyboard). The Spin was mapped to the Return key.</p>

<p>Now, the next challenge was to find one or more games which are engaging and require low bandwidth. ‘Low bandwidth’ because we knew the precision of detecting neck gestures might not be that perfect, and we might only be able to capture certain movements confidently. We shortlisted to games as shown in picture. From the world famous nostalgia Mario Bros., recently famous Flappy bird and 2048, to which the whole programming community is currently hooked. The game play of all these games is fairly simple; they all require 4 key inputs or less.</p>

<p><img src="http://hcid.us/images/prototyping/fp/mario.png" width="266"><img src="http://hcid.us/images/prototyping/fp/2048.png" width="266"><img src="http://hcid.us/images/prototyping/fp/bird.png" width="266"></p>

<p><em>Basic yet famous games that we used to behaviour prototype. From, Left to Right: 2048, Mario Bros, Flappy Bird.</em></p>

<p>We came up with quite a few gesture sets and games as said before that  could possibly map to one or more games. To choose one, we went on to Behaviour prototype all sets of gesture inputs. This gave us valuable findings and insights into advantages and disadvantages of each set of neck gestures. Now for the running the prototype, we first discussed about setting up the system, which allowed quick iterations and modifications on the fly. We used an external Keyboard to input commands as the user performed the gestures from the given set. We also took latency into consideration, for which we triggered the input with a small delay to account for technical latency and give more real-like experience. This Wizard of Oz technique, worked quite well highlighting problems.</p>

<h3>Electronics Prototype</h3>

<p>The behavior prototyping gave us clear foundation to take the process to higher fidelity. We choose to technically prototype it using an Arduino Do-It-Yourself development kit. This way we not only make a prove of concept and technically prove it, but also give an almost end-product like experience. It is a great way to run evaluation, as we get more applicable feedback and analysis.</p>

<p>To develop this prototype we also used a 3-axis Accelerometer sensor to track acceleration in all three axises. The accelerometer sat in wearable head mounted cap, in which it was embedded. We also used a 9V battery pack to power the Arduino board and two X-bees to make the system portable. X-bees uses serial communication to transfer data to other X-Bees wirelessly in the same network. One X-bee was attached to the computer running Processing and the other X-Bee was attached to the RX &amp; TX of the Arduino board for transmitting the sensor data and receiving some commands respectively.</p>

<p>The received data is used to detect gestures using Processing. The gravity plays a major part is helping us differentiate the acceleration signal. Once we detect a gesture a system command is issued for a System Event regarding a key pressed. All four gestures generate their respective system event.</p>

<p><img src="http://hcid.us/images/prototyping/fp/prototyping1.jpg"></p>

<p>The Processing not only just issues system events it also generates sound upon a successful gesture. This is important for feedback. The sound confirms the event been taken place, and prevents the user from performing the gesture again.</p>

<p>Also, for better recognition of gestures we added a feature to calibrate the sensing. This enabled custom thresholds for each gesture. To calibrate, a user just have to move the to the position and press either ‘W’,’A’,’S’ or ’D’ keystroke depending upon the gesture. Though, it comes with a default setting that works in most cases.</p>

<p><img src="http://hcid.us/images/prototyping/fp/prototyping2.jpg">
<em>The Arduino Board, with X-Bees and Accelerometer sensor on a separate breadboard.</em></p>

<h2>User Testing</h2>

<h3>Testing Behavior Prototype</h3>

<p>Initially, we conducted an exploratory user testing with two participants. We expected to explore the interaction between neck movement and a computer game: what kind of neck gestures are needed for exercise? which of them are proper for an immersive game?. We have predefined a set of neck gestures and apply them to three different computer games.</p>

<p>We provided them with a laptop computer with a game initiated and the related instructions for that game. Then the user will start playing the game via neck gestures. Because the performance of the game is actually not the most important feature, so we didn’t give the user any specific task such as finishing a level in the game. Instead we are more interested in whether such an environment is fun and engaging for the user. Therefore, we ask them to just play with the game and enjoy it, and we will evaluate how much they like playing the game, and whether it is a significant neck exercise.</p>

<h3>Evaluating Behavior Prototype</h3>

<h4>Type of Game</h4>

<p>We found that not all games are appropriate for controlled using neck movement. Games like Flappy Bird have a really fast tempo, and need constant input to control the game which really intimidates the user. Because when they are doing a neck movement, their field of vision will be temporarily moved to the outside of the screen, and if they want to continually focus on the game, they have to do the gesture very fast. But moving neck fast is actually not good for exercise purpose and also have potential danger of injuring the neck.</p>

<h4>Choice of Interaction</h4>

<p>An interesting finding is that our neck and head are physically working like a joystick, which provides us a perfect mapping of game controller. One thing we confused about is the mapping of left-right arrow key. Some people think shaking head are more intuitive while others think swinging is better. We found technically it’s really hard to figure out to which direction the command is when user shaking their head, so we choose swinging and start building our electronic prototype.</p>

<p><img src="http://hcid.us/images/prototyping/fp/usertest.jpg"></p>

<h3>Testing Electronic Prototype</h3>

<p>After building up the electronic prototype, we integrated it with the OS X system and allow it to send command keys to three different games. The same as we did with the behavior prototype, we asked the user to wear the hat with the sensor, introduce all the rules and purpose of this system, and then allow them to play with three different games.</p>

<h3>Evaluating Electronic Prototype</h3>

<h4>Tolerance of Misinterpretation</h4>

<p>Although we tried many method for improving the accuracy of gesture recognizing, in the best case, the system can achieve around 90% precision at recognizing the users’ gesture. We found for the game 2048, occasionally random keystrokes are acceptable for gamers, which actually bring more fun to this game. But for games like Super Mario and Flappy Bird, users need more accuracy in terms of input, because it’s intense and need meticulous controller.</p>

<h4>Wireless Or Not?</h4>

<p>We build our first electronic prototype with XBees to make it wireless with the intention that people may wear a hat and play the game wirelessly. Although it might be a good idea to make it wireless for end product, we eventually remove XBees and test it with a wired version of our prototype. The main reason is that XBees have relatively less community support and for some reason, it continually stop working which we can’t event debug to find where is the problem. The other reason is that for exploring the gaming environment and testing the interactions, it really doesn’t matter if it’s wireless or not. A longer wire can simulate ‘wireless’, and it also makes the prototype much simpler.</p>

<p><img src="http://hcid.us/images/prototyping/fp/studio.jpg"></p>

<h2>Synthesis</h2>

<p>We felt that our behavior prototype was successful in providing a “quick and dirty” way to test different gesture sets that we developed. We learned a lot not only about the user’s gesture preferences, but also about the level of accuracy that the gesture recognition technology would need to account for. After that, we did an electronic prototype to provide a proof of concept and also to prove its feasibility. The user testing helped us focus by eliminating features/other solutions and reach a conclusive solution to the problem. If we were to do it all over again, we would also do a heuristic evaluation of the interface.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Physical Prototyping with Rhino 3D]]></title>
    <link href="http://hcid.us/blog/2014/03/09/physicalprototype/"/>
    <updated>2014-03-09T03:10:40-07:00</updated>
    <id>http://hcid.us/blog/2014/03/09/physicalprototype</id>
    <content type="html"><![CDATA[<p>For our eighth prototyping project in HCID 521, we were asked to create a 3D model using Rhino 3D, and printer it with 3D printer. As I’m fairly new to Rhino, and Rhino is a really complex software, this assignment is probably the hardest one for me so far.</p>

<p>I got my basic understanding of Rhino and its interface from the lecture that Kerem gave us. And I also tried 3D printing machine in the studio which really excites me.</p>

<p></hr></p>

<p><img src="http://hcid.us/images/prototyping/a8/exercise.jpg" width="390"><img src="http://hcid.us/images/prototyping/a8/mhcid.jpg" width="390"></p>

<p>Inspired by all those fancy <a href="https://www.google.com/search?q=3d+printed+guitar&amp;tbm=isch">3D printed guitars</a>, I really want to make a 3D printed instrument by myself. But making a guitar might be too much for a Rhino beginner to accomplish in one week, therefore I decided to make another instrument that I like a lot, the harmonica. I made it by referring to the SUZUKI harmonica I have, and it actually helped a lot by measuring the size of each parts.</p>

<p><img src="http://hcid.us/images/prototyping/a8/rhino.png"></p>

<p><img src="http://hcid.us/images/prototyping/a8/perspective1.png"></p>

<p><img src="http://hcid.us/images/prototyping/a8/perspective2.png"></p>

<!-- more -->


<p>The knowledge and experience gained from this assignment is extremely valuable for me. Not only I have learnt a lot about using the software, but also I start to understand the process, constraints and subserviency of industrial design.</p>

<p><img src="http://hcid.us/images/prototyping/a8/exhib.jpg"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Electronics Prototype: Pomoduino Timer]]></title>
    <link href="http://hcid.us/blog/2014/03/03/electronics-prototype-pomoduino-timer/"/>
    <updated>2014-03-03T14:10:25-08:00</updated>
    <id>http://hcid.us/blog/2014/03/03/electronics-prototype-pomoduino-timer</id>
    <content type="html"><![CDATA[<h2>Background</h2>

<p><img class="right" src="http://hcid.us/images/prototyping/a7/clock.jpg" width="230"></p>

<p>Building interactive physical systems is a great joy to me. It’s always fun to design the circuits, solder different parts and make things work. This week, our prototyping studio class gives me a chance to build an electronics prototype with Arduino.</p>

<p>As there is only one week for me to do this assignment, I decide to make something useful to me and also small enough so that I can handle it in one week. Looking at my dumped alarm clock, I decide to build a Pomodoro Timer out of it.</p>

<p><a href="http://en.wikipedia.org/wiki/Pomodoro_Technique">Pomodoro Technique</a> is a popular time management method. Based on the idea that frequent breaks can improves mental agility, this method breaks down the work into intervals, 25 minutes in length, and separated by short breaks.</p>

<p>To apply Pomodoro Technique in real life, people invented the Pomodoro Timer which is similar to a kitchen timer, but can only time 25 minutes every time. The physical act of winding up the timer confirms the user’s determination to start a task, the ticking to externalises desire to complete the task, and the ringing announces a break.</p>

<p>For this prototyping exercise, I made a pomodoro timer mainly with an Arduino UNO board, a LCD screen, and a motor.  The LCD screen is used to visualize the ticking and how much time left in the running pomodori. The button is the only way to interact with the system which can toggle the status between running and reseted.</p>

<!-- more -->


<p>Here is a demo video of the pomodoro timer I made, which I call it <em>pomoduino</em>:</p>

<div class="video-container">
<iframe src="http://hcid.us//player.vimeo.com/video/88049469" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
</div>




<hr>


<h2>Playing With Electronics And Arduino</h2>

<h4>Motor Module</h4>

<p><img src="http://hcid.us/images/prototyping/a7/motor1.jpg" width="400"><img src="http://hcid.us/images/prototyping/a7/motor2.jpg" width="400"></p>

<p>The motor module is used to power the bell in the alarm clock. I firstly prepare and test the circuits with an extra motor and migrate it to the motor inside the clock in the end.</p>

<p>The transistor acts like a switch to control the motor; it make use of the signal from Arduino to power another circuit which much higher current. The diode and capacitor in this circuit are meant to protect the circuit from noise and spikes.</p>

<h4>LCD Screen Module</h4>

<p><img src="http://hcid.us/images/prototyping/a7/screen1.jpg" width="400"><img src="http://hcid.us/images/prototyping/a7/screen2.jpg" width="400"></p>

<p>The 16 pins LCD screen is widely used with single-chip microcomputers. With arduino, the library and driver for controlling this LCD is actually pre-installed in the Arduino IDE. The challenge here is to neatly layout all these wires and present a clear interface to the user. I bundled all the wires to go through the hole in the center of the clockface, and stick the small breadboard on the surface.</p>

<h4>Button Module</h4>

<p><img class="right" src="http://hcid.us/images/prototyping/a7/button.jpg" width="300"></p>

<p>The button here actually works like a switch that can change the state of the system. The system have to keep track on the current and previous value of the button, and only change the state of the system when its value change from LOW to HIGH. That means the state of the system will only be changed when the user push down the button. A tutorial on this could be found <a href="http://www.arduino.cc/en/Tutorial/Switch#.UxegtPRdXHA">here</a>.</p>

<h4>Putting Everything Together</h4>

<p>After testing each part, I start to assemble different parts of the hardware together. This is the overall layout of the circuits:</p>

<p><img src="http://hcid.us/images/prototyping/a7/circuits.jpg"></p>

<p>I apply a 9 volts battery to power both the arduino and the motor. The input-output interface(button, LCD screen and a breadboard) is separate from the others, and is positioned in the front of the ‘clock’. Another breadboard is attached to the arduino board, together in the back of the ‘clock’, and used to arrange the motor circuits. I removed the original wires that connected to the motor inside the clock, and soldered another two that is easier to connect to the breadboard.</p>

<p><img src="http://hcid.us/images/prototyping/a7/alltogether.jpg" width="400"><img src="http://hcid.us/images/prototyping/a7/back.jpg" width="400"></p>

<h2>Reflections</h2>

<h4>Constraints Of Prototyping</h4>

<p>During prototyping electronics, the components we have are usually not ideally to our design. In this project for example, I want to present ‘push button to start’ on the LCD screen. But the LCD is not long enough to contain this sentence so that I change it to a less appropriate one: ‘click to start’.</p>

<p>I recognized that there are always lot of constraints in prototyping electronics, because we can’t get all the hardware parts customized to our need. But most of them it doesn’t matter to the prototype itself, as long as we can continue the following usability testing upon the prototype. Some walkaround could help to build the initial prototype.</p>

<h4>Software Engineering Principles</h4>

<p>Building and debugging hardware seems very challenge to me in the beginning. Until I find that I can actually apply my knowledge in developing software to building electronics. The principle of Modular Programming, Encapsulation and API design are also quite useful in building hardware. For example, if I haven’t divided the circuits into separate modules in the beginning, it would take much more effort to deal with all those wires, and also much harder to debug potential issues in hardware or software.</p>

<h4>Be Safe &amp; Follow Best Practice</h4>

<p>Playing with electronics is not safe especially when high current circuits are involved in your project. Even with only 9 or 5 volts battery, it’s really easy to break some components in the circuits. We can’t expect every prototyper to be an expert in digital design and create appropriate protect circuits. But as a prototyper, it is important to follow the best practice from all those tutorials and examples, so as to avoid those dangers.</p>

<h4>Fast Prototyping with Arduino</h4>

<p>Building prototypes with Arduino is so fast. There is a huge and active online community that contributes to millions of libraries and tutorials. You can easily get start with any type of sensor or components, by just following the tutorials, and use related libraries. The processing language also allow the user to integrate their application with other programming language or applications. I think this is the reason that Arduino is a good choice for prototyping electronics. The less effort you spend on technical problems, the more time you could focus on solving the real problem.</p>

<p><img src="http://hcid.us/images/prototyping/a7/books.jpg" title="center" ></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mobile Prototype: Tweak The Tweet]]></title>
    <link href="http://hcid.us/blog/2014/02/28/mobile-prototype-tweak-the-tweet/"/>
    <updated>2014-02-28T11:30:05-08:00</updated>
    <id>http://hcid.us/blog/2014/02/28/mobile-prototype-tweak-the-tweet</id>
    <content type="html"><![CDATA[<h2>Background</h2>

<p><img class="right" src="http://hcid.us/images/prototyping/a6/designspecification.jpg" width="230"></p>

<p>This week, our HCID 520 prototyping class is ready to extend our prototyping skills to building mobile applications. Our task is to build a working prototype of the TtT(<a href="http://faculty.washington.edu/kstarbi/tweak-the-tweet.html">Tweak The Tweet</a>) mobile app, to support future usability study.</p>

<p>TtT is an application that is part of HCDE Professor <a href="http://faculty.washington.edu/kstarbi/">Kate Starbird</a>’s work in crisis research. She studies the “use of social media during crisis events, specifically looking at how the converging audience (aka, the ‘crowd’) can contribute—and is already contributing—to crisis response efforts.“</p>

<p>TtT uses Twitter to gather and direct information during crises to people who can act on it to the benefit of affected people and communities. And TtT aims to allow digital volunteers to provide information in a form that is more easily and reliably processed and analyzed.</p>

<p>We are given the design specification of this application that written by Grace Jang. It includes all the user interfaces, interactions and functional requirements of this app. And the prototype is suppose to be consistent with the specification.</p>

<p>Here is a short video demo about this mobile prototype I developed:</p>

<div class="video-container">
<iframe src="http://hcid.us//player.vimeo.com/video/87334846" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
</div>




<!-- more -->


<h2>Prototyping</h2>

<p><img class="left" src="http://hcid.us/images/prototyping/a6/phonegap.jpg" width="270"></p>

<p>There are lots of tools for prototyping high fidelity mobile applications. Considering the scale of this application, non-programming approaches, such as App Inventor or Axure, might be very hard to manage and even harder to adapt to change in future works. So I choose PhoneGap(also known as Apache Cordova) and web technology to build this prototype since I am already very familiar with web programming.</p>

<p>PhoneGap is a mobile development framework that allows software engineers to build application for mobile devices using web technologies(JavaScript, HTML5, CSS3). It use a middle layer that contains a webview inside a native app to render the UIs, and integrate many system service and apis into corresponding JavaScript functions.</p>

<p>Taking advantage of using web, I can create highly customized user interface using HTML and CSS. And thanks to jQuery Mobile and Bootstrap, creating the UIs defined in the specification is not hard at all. Here is some screenshot of the UIs I made using web:</p>

<p><img src="http://hcid.us/images/prototyping/a6/1.jpg" width="266"><img src="http://hcid.us/images/prototyping/a6/2.jpg" width="266"><img src="http://hcid.us/images/prototyping/a6/3.jpg" width="266"></p>

<p><img src="http://hcid.us/images/prototyping/a6/4.jpg" width="266"><img src="http://hcid.us/images/prototyping/a6/5.jpg" width="266"><img src="http://hcid.us/images/prototyping/a6/6.jpg" width="266"></p>

<p><img src="http://hcid.us/images/prototyping/a6/7.jpg" width="266"><img src="http://hcid.us/images/prototyping/a6/8.jpg" width="266"><img src="http://hcid.us/images/prototyping/a6/9.jpg" width="266"></p>

<p>I started with build a mobile web application: using a local HTTP server to host the application and do testing in the browser. PhoneGap also provides a toolkit for developers to create all the files you need to pack up the web app into a native app installer. This allow me to use XCode to load the project folder generated from PhoneGap, and install this application on an iOS emulator.</p>

<p><img src="http://hcid.us/images/prototyping/a6/vim.jpg" width="400"><img src="http://hcid.us/images/prototyping/a6/xcode.jpg" width="400"></p>

<h2>Reflection</h2>

<p><img class="right" src="http://hcid.us/images/prototyping/a6/login.jpg" width="230"></p>

<p>One thing confused me a lot in the beginning is the authentication page in the design specification. It defined a customized login page for twitter and suppose that page can be implemented. But in fact, this kind of customized login page only exist before Twitter canceled their support for <code>HTTP Basic Auth</code> due to security reason. Nowadays twitter only support <code>OAuth</code> for authentication, which means the mobile app will redirect to a web page provided by twitter and not allowed to store the users’ password.</p>

<p>It’s possible that the author of this design specification didn’t discuss its design with a developer in detail. So that he or she can not really recognize all the constraints in designing this application. Moreover, since this app only need twitter authentication to post a tweet, it’s more appropriate to use <a href="https://dev.twitter.com/docs/ios/using-tweet-sheet">Twitter Sheet</a> or for PhoneGap, the <a href="https://dev.twitter.com/docs/intents">Web Intents</a>. Because in that case, the application itself don’t need to worry about the authentication.</p>

<p>In general, I think prototyping TtT app using the web is a good choice. Comparing to developing native application, developing a mobile web application is arguably cheaper and faster, which is exactly what we expect from a prototyping tool. Although there are some constraints of building mobile app using web, including offline functioning, system level features, and UI responding speed, but the TtT app doesn’t require any offline feature or system level apis. Also for prototyping, the speed of webview is totally acceptable.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Website Prototype: dub Site Redesign]]></title>
    <link href="http://hcid.us/blog/2014/02/15/website-prototype-assignment-dub-site-redesign/"/>
    <updated>2014-02-15T16:57:15-08:00</updated>
    <id>http://hcid.us/blog/2014/02/15/website-prototype-assignment-dub-site-redesign</id>
    <content type="html"><![CDATA[<h2>Background</h2>

<p>Our weekly prototyping assignments started with low fidelity prototyping to higher fidelity ones. This week we are proposed to build an interactive wireframe for a website prototype. We are giving an existing website(our very own dub website), to analyze its current content and functionality and redesign the information architecture, navigation and general layout.</p>

<h2>Design</h2>

<p><img class="right" src="http://hcid.us/images/prototyping/a5/dub-site.png" width="300"></p>

<p>After an extensive research and analysis on the current dub website (dub.washington.edu/), I got a basic understanding of all the functionality and content of this site. And synthesis with three user interviews about their experience of using this website, I have the following findings about users’ usual practice of using this site:</p>

<ul>
<li>Calendar is the most regularly used functionality for dub members, to track the newest events, specially the weekly dub seminar;</li>
<li>For other audience who want to get to know about dub community, the people and their publications/projects are the most visited pages;</li>
<li>The blog is less popular among visitors which may because of its lack of updates;</li>
</ul>


<p>According to all these findings, I think there are several main problems in terms of information architecture:</p>

<!-- more -->


<ul>
<li>The most regularly used information(events, seminars) is located in secondary pages instead of homepage, and the upcoming event is not being highlighted.</li>
<li>There are some huge list of names/titles of people/publication/project which makes it difficult to navigate through. A better index or search function is needed.</li>
<li>The connections between dub people, and their publications and projects are not effectively presented.</li>
</ul>


<h2>Prototyping Process</h2>

<p>To address these problems, I redesigned the information architecture of this site while remaining most of the functionalities of the original website. Here I will introduce my new design:</p>

<h3>Home Page</h3>

<p>I use the masthead area to present the most important news and announcements. I envision this is a dynamic area with a wide image as background, so as to give new visitors a visualized impression of dub.</p>

<p>Following the masthead, is a selected partition of the most recent blog posts and news with some thumbnails and a detailed text paragraph of introduction with a link to view the whole article. Here I wish to present all the must-know news, announcements as well as the upcoming dub seminar, so that dub members could get to know about the weekly talk directly from the home page.</p>

<div class="center">
<img src="http://hcid.us/images/prototyping/a5/index.png" width="380"><img src="http://hcid.us/images/prototyping/a5/drop-down-menu.png" width="380">
</div>


<p>The DUB logo on the top left corner is the top level menu across the whole site in the format of a hover to drop-down menu. User can use this menu as a shortcut to quickly redirect to the target resource.</p>

<h3>People/Publications/Projects Page</h3>

<p>The DUB people page lists all the dub members and provides a link to their own page. The current website has a huge list of names with only their department. It’s actually very annoying for new visitors to get to know about the people here. Take my experience for example, to find a professor interested in the field of data visualization, I have to press ‘command’ key and click on all the links, and then check them one by one.</p>

<p>It will be much better if we can show some rudimentary information about the professor in the listing page. But the drawback of this approach is you need to scroll down the page a lot to find a specific one.</p>

<p>So in my design, I try to combine both approaches where I put the name list on the left as a quick reference. And put a list of short introductions with a portrait image on the right side. In order to navigate to a certain people faster, I made the reference list a folding menu list because it’s actually a very long list referring to the current website.</p>

<p><img class="center" src="http://hcid.us/images/prototyping/a5/people-list.png" width="500"></p>

<p>For the current dub site, I found these are a lot of links between the dub people, the projects and the publications. But by click on a link and jump to another page, users can’t really percept or memorize all the connections easily. They are connected but visually disconnected.</p>

<p>Take into account this observation, I designed this vertical accordion page to show each person’s related publications and projects. There are three columns in this accordion page where the introduction page is unfolded by default. When the user click on the publications or the projects, the corresponding partition will unfold and the previously unfold column will be enfolded:</p>

<p><img class="center" src="http://hcid.us/images/prototyping/a5/accordion.gif" width="500"></p>

<p><img src="http://hcid.us/images/prototyping/a5/people-name.png" width="266"><img src="http://hcid.us/images/prototyping/a5/people-publications.png" width="266"><img src="http://hcid.us/images/prototyping/a5/people-projects.png" width="267"></p>

<h3>Calendar Page</h3>

<p>I personally don’t like using such calendar view to show recent events, but most dub members are already getting used to this approach and the current calendar page is actually the most visited page according to the survey. So I keep the same functionality in the new design, and put the entrance on the upper right corner of the navbar for faster accessing.</p>

<h2>Prototyping Tools</h2>

<p>To create such an interactive wireframe prototype, I have several different tool to choose from: using HTML/CSS or using a specification prototyping software such as Axure, Balsamiq etc. As I’m already very familiar with developing websites, I choose to learn and use Axure this time so as to learn something new.</p>

<p>Building the home page in Axure is pretty easy. I made it without reading any extra documentation or tutorial. Because most elements in the homepage are very basic, and the interaction with those elements are very simple. But when I started to build the people profile page, it becomes very tricky. I searched for several implementation of accordion menu in Axure, but they are neither too expensive, nor have no documentation or any clue for you to reuse it. To work around this problem, I build three different pages and use link to simulate the accordion page, but the transition is too slow.</p>

<p><img class="right" src="http://hcid.us/images/prototyping/a5/axure.png" width="400"></p>

<p>Another thing about axure is that comparing to using template in web development, it’s really hard to reuse partition of the page in other pages. I have to copy the navbar, the footer, and the navigation menu on every page. And each time I want to update a link in the menu, I have to change it on every page, which is very annoying.</p>

<p>After creating this prototype in Axure, I guess I will not use Axure again. My reason is that comparing using HTML/CSS/Javascript to using Axure, the only drawback is the learning curve is relatively higher. However, for someone who is already familiar with web developing, there is no need to learn Axure. Actually with the help of countless open source components and frameworks(Bootstrap, Blueprint) from the web development community, a developer can build the prototype very quickly.</p>

<p>Another reason is that for only creating the wireframe, using Axure may be easier. Nevertheless, to learn to implement some advanced interaction in Axure, it may take even more time than the HTML/CSS/Javascript approach.</p>

<p>Thanks to Axure, I can publish the prototype online: <a href="http://fik3lo.axshare.com/.">http://fik3lo.axshare.com/.</a> Not all the links in this page can work due to time constraint.</p>

<h2>Reflection</h2>

<p>Here is some feedbacks I collected from other classmates:</p>

<ul>
<li><p>Drop down menu on the top left corner may need some improvement. There is plenty space on the top of the page for all the links, replace it using a navigation bar with link to ‘home’, ‘people’, ‘publications’, will be better.</p></li>
<li><p>No one knows how to use the Search on the top right corner. I think it’s better to add a tooltips for the search functionality.</p></li>
<li><p>Didn’t consider the situation where the faculty or student don’t have any publications/projects. The page will definitely change a lot or have some blank.</p></li>
</ul>


<p>I also created a video to introduce my design via recording people using this prototype. Most of the interaction in the video in understandable to audiences, and they can get the idea of showing the connections between people and their publications/projects through a single page accordion menu. Nevertheless, my tester got confused when he was using the prototype alone. The page redirection transition makes it harder to perceive this idea.</p>

<p>Here is the video I made to present this website prototyp:</p>

<div class="video-container">
<iframe src="http://hcid.us//player.vimeo.com/video/86963982" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
</div>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Behavior Prototype: Speech-to-Text Dictation System]]></title>
    <link href="http://hcid.us/blog/2014/02/08/behavior-prototype-assignment-speech-to-text-dictation-system/"/>
    <updated>2014-02-08T13:23:00-08:00</updated>
    <id>http://hcid.us/blog/2014/02/08/behavior-prototype-assignment-speech-to-text-dictation-system</id>
    <content type="html"><![CDATA[<blockquote><p>This article and its related work is developed by Hadiza Ismaila, Chaoyu Yang, and Chase Chen-Hung Wu.</p></blockquote>

<p>In order to learn about user’s behavior of intuitive verbal commands on speech-to-text system, we designed and implemented a rapid behavioral prototype for user testing. Built by Google Drive with a person playing the speech-to-text system (a.k.a. Wizard of Oz), our rapid prototype was tested and modified for several rounds with approaches such as simple text input, editing, error handling, and cursor moving. We summarized and organized the result into different categories of interactive patterns, and came up with valuable learnings and insights from our result analysis.</p>

<div class="video-container">
<iframe width="640" height="360" src="http://hcid.us//www.youtube.com/embed/p3B-Bi3ZO8g" frameborder="0" allowfullscreen></iframe>
</div>




<!-- more -->


<h2>Background</h2>

<p><img class="right" src="http://hcid.us/images/prototyping/a4/draft.png" width="250"></p>

<p>Our task for this assignment is to create a behavioral prototype to explore the user interaction scenario of three choices: Speech to text dictation, Home alarm system, or Gesture recognition platform. After extensive research on all three topics, our group decided to work on speech-to-text dictation system, considering it as an inspiring and challenging topic.</p>

<p>The scenario for our testing is the user using speech to text dictation application that makes use of voice recognition to create and edit simple documents. This includes using spoken language to input text and using verbal command for simple text editing tasks.</p>

<p>We were particularly interested in exploring the following two questions through the user testing:</p>

<ul>
<li>Expectation of verbal commands for text editing tasks</li>
<li>Tolerance for errors during dictation.</li>
</ul>


<p>We expected to use low-cost, low-tech materials to build the prototype, in order to speed up our explore our testing and modification process, and also achieve the goal of answering user experience questions. The prototype should allow us to modify rapidly, which gives us the flexibility to control and compare the variable we want to observe. Last but not least, we would like to create a realistic interactive experience of our prototype, just like any user using a real dictation application during testing.</p>

<h2>Prototype</h2>

<p>While planning out our prototype, we came up with three approaches of building it:</p>

<h4>Google Drive</h4>

<p><img class="left" src="http://hcid.us/images/prototyping/a4/prototype.png" width="300"></p>

<p>The first approach is using Google Drive (Google Doc) to type down the speech manually. The user will be asked to giving verbal commands to a laptop that shows a window of Google Doc, while a VoIP application running in the background that connects our wizard. Our wizard would be one of our teammate, acting as a part of our prototype: sitting somewhere else, using VoIP to hear the user’s voice, and typing down anything he heard on the document, which will also show up on the user’s screen.</p>

<p>Our concern of this prototype is that our typing speed is much slower than an actual dictation program. Plus the tooltips on the typing cursor showing the typist’s name, a user can easily tell it’s someone typing in the background.</p>

<h4>Simple Web Program</h4>

<p>The second approach is to build a simple web program, which allows you to prepare a paragraph of text, and pop up word by word in the testing. The tester will see the web program in an extended monitor with another operator in the other side, listening to Skype voice call, inserting words by clicking and modifying text manually.</p>

<p>The problem with this prototype is that we need to ask the user to input a specific paragraph of text. And when we are modifying the text, the user can see the mouse and cursor moving, which makes it seem less realistic.</p>

<h4>Google Slides</h4>

<p>The third idea is using Google Slides to pop up the pre-defined paragraph word by word while the user speaking. It’s very similar to the second approach while it also allow us to do more complicated modification during the testing. However, the drawback is that it’s hard to modify anything in the middle of the testing because you can’t just create new slides in front of the user, and if you change one of the slides, it will be discontinuoud to the next ones.</p>

<h4>Decision: Google Drive, &amp; Setup</h4>

<p><img class="right" src="http://hcid.us/images/prototyping/a4/decision.jpg" width="300"></p>

<p>After trying out different approaches and comparing their performance, we decided to choose the Google Drive one, since it provides more flexibility for us than other designs to modify prototype in short time, which helps up explore the users’ expectation of verbal editing commands in both greater width and depth.</p>

<p>Part of our prototype was connectivity, and we applied FaceTime to do so. We made FaceTime run in the background to transfer user’s sound to our wizard, while our wizard typing down words manually on Google Doc from another laptop. We were hoping this would make the prototype look like a real system or provide environment similar to a real system.</p>

<p>We were attempting to convince our users that they are testing some part of a real dictating application. Below shows how we created the context:</p>

<ul>
<li>Our wizard and users were staying in different rooms, and the tester would not be (or be less) aware of typing sound by wizard.</li>
<li>Since we were using Google Drive as a main role of our prototype, we told our user that our system is an extension of Google Drive. We readjusted the display of Google Drive, and put on an annotation, “Google Drive Dictation System / Trial”, to make user ignore the low fidelity parts of prototype.</li>
<li>We typed down anything that the user said during the test, pretending to be a not-smart-enough system.</li>
<li>During the test, we arranged and applied several possible mistakes made by system, in order to observe user’s error handling.</li>
</ul>


<h2>Testing 1</h2>

<p><img class="left" src="http://hcid.us/images/prototyping/a4/testing1.png" width="300"></p>

<p>Initially, we conducted an exploratory user test on two participants. We expected to explore users’ reaction to general functions and events of a speech-to-text system: structure of users’ verbal commands, error handling behavior during dictation, and complexity of their commands (system’s perspective). Therefore our prototype had no pre-defined set of verbal commands, which gave the users flexibility to use any verbal command of their choice.</p>

<p>We provided them with a short passage to dictate to the speech-to-text system. Users were also asked to perform the following tasks and to ensure that each sentence was correctly generated by the system.</p>

<h4>Passage</h4>

<p>The giant panda is perhaps the most powerful symbol in the world when it comes to species conservation. This peaceful, bamboo-eating member of the bear family faces a number of threats. It’s forest habitat is fragmented and populations are small and isolated from each other.</p>

<h4>Task</h4>

<p>All operations should be verbal commands. No other available input methods.
* Input the first sentence
* Input the second sentence
* Input the third sentence
* Replace the second sentence with Existing pandas are facing lots of threats.</p>

<p>The wizard typed down the sentences while users spoke. He/she would generate predefined mistakes in the sentences, which enabled users to provide verbal commands to correct the mistakes.</p>

<h2>Evaluation 1</h2>

<h4>Dictation/Error Recognition</h4>

<p><img class="right" src="http://hcid.us/images/prototyping/a4/evaluation1.png" width="300"></p>

<p>Since the users read from a passage on a paper, they dictated at a quite fast pace and paid more attention to the text they were reading than the text being displayed on the screen. They only noticed a mistake made by the system after they were done reading a particular sentence. They reviewed the generated text in order to find mistakes and give commands to the system to correct them.</p>

<h4>Complexity of Verbal Commands</h4>

<p>One interesting finding was that the use of natural language in verbal commands. Some of their commands for editing was far from trying to match operations that are normally provided by keyboard or mouse. Instead of regularized/robotic commands, they operated in phrases and complete sentences rather one word.</p>

<p>Following are the examples of verbal commands they used in correcting errors made by the system:</p>

<ul>
<li>Change consolation to conservation.</li>
<li>No, the bamboo is not pregnant. it is fragmented.</li>
<li>Instead of stress put threats.</li>
<li>Replace stress with threat.</li>
<li>Correct et to eating.</li>
<li>Add a comma after peaceful.</li>
<li>Replace pregnant with fragmented.</li>
<li>Delete the second sentence.</li>
</ul>


<h4>Same Function, Different Commands</h4>

<p>Also, users used different commands for correcting same mistakes in a particular scenario. They expected the system to understand or accommodate multiple commands for the same function instead of using one specific command.</p>

<h4>Improvement in Next Iteration</h4>

<p>At this round we found users frequently questioning about the authenticity of the system, since the system only responded to the speech we assigned them. We couldn&rsquo;t evaluate user tolerance for error handling because the errors were few and the wizard responded to any command the user gave. Additionally, since the users were restricted to a set of instructions, the commands were only limited to changing a particular word or sentence.  The test was not sufficient enough to get meaningful answers to our questions. As a result, we refined our prototype and conducted further testing.</p>

<h2>Testing 2</h2>

<p><img class="left" src="http://hcid.us/images/prototyping/a4/testing2.png" width="300"></p>

<p>We found two other users for our second round of testing. We provided them with the same passage and made a few changes to the task.</p>

<h4>Task</h4>

<ul>
<li>Input the first sentence</li>
<li>Input the second sentence</li>
<li>Input any sentence of your choice</li>
</ul>


<p>This time, the wizard responded to not only the input text dictated but every word the user said to make it seem more believable. Also, the wizard deliberately doesn&rsquo;t respond to some error correction commands made by the user to test what other ways the user might give commands for correcting the same mistake. We wanted to explore more commands a user would use and see if there were predictable patterns of commonly used commands.</p>

<h2>Evaluation 2</h2>

<p>With adding a random scenario and typing everything uttered, users were convinced that this was an authentic system.</p>

<h4>Dictation/ Error Recognition</h4>

<p>In this round of testing, we noticed that users paid more attention to the screen when dictating a random sentence to the computer and could easily identify errors made by the computer and make just-in-time corrections. Also, they dictated in a slower pace than they did when reading from a text.</p>

<h4>Similar commands</h4>

<p><img class="right" src="http://hcid.us/images/prototyping/a4/evaluation2.png" width="300"></p>

<p>We also noticed similarity of words and phrases used as voice commands for editing and correcting an error across all users.
Commonly used phrases include:</p>

<ul>
<li>Change incorrect word to correct word</li>
<li>Replace incorrect word to correct word</li>
<li>Delete incorrect word and replace with correct word</li>
<li>Correct incorrect word with correct word</li>
</ul>


<h4>Error Handling / Correction</h4>

<p>When correction was done in real time, the user simply repeats the word until the system gets it right. If after multiple tries, the system doesn’t understand or respond to their command, they spell out the word to the system.</p>

<h4>Improvement in Next Iteration</h4>

<p>Although making changes made our system more authentic and enabled us to get more insights on similar commands, The results were not very different from the previous test and the commands were also still limited to the change and delete functions. We realized that giving the user predefined text to read from wasn&rsquo;t so effective and decided to perform another test without any predefined text or task.</p>

<h2>Testing 3</h2>

<p><img class="right" src="http://hcid.us/images/prototyping/a4/testing3.png" width="300"></p>

<p>For the final round of testing, we simply asked our user to speak any sentences that came to mind gave them the freedom to do any editing task. Here, the wizard kept generating many random and repetitive errors in order to have a better understanding of how tolerant a user can be of errors generated by the system.</p>

<h2>Evaluation 3</h2>

<p>By putting the user in a more flexible scenario, we got more insights on verbal commands for other functions such as moving cursors, copy and paste and navigating to a particular line.</p>

<h4>Moving cursors</h4>

<p>User came up with the following commands to navigate the cursor to a desired location for editing.</p>

<ul>
<li>go up &ndash; To move cursor up</li>
<li>go down &ndash; To move cursor down</li>
<li>go back &ndash; To move cursor left</li>
<li>go to 4th line, delete this line</li>
</ul>


<h4>Cut and Paste</h4>

<p>Surprisingly, the user used the cut and paste function to move a particular sentence to the end of the document. At first the user gave a complex command:</p>

<blockquote><p>“Cut this line and paste it in the end of the document.”</p></blockquote>

<p>But when the user realized that the system was unresponsive to her command she split the sentence in two separate command and said each command one after the other.</p>

<h4>Error Handling</h4>

<p>Because the system kept generating random and repetitive errors,  the user kept repeating commands for a while before the system responded to the command which frustrated them.</p>

<h2>Conclusion</h2>

<p>From the testing, we can deduce that the set up of our prototype the use of google doc and facetime made it effective. Learning from each test and iterating and refining our design helped us to gain valuable knowledge about users expectations of verbal commands and their tolerance for errors in a simple dictation/editing task.</p>

<h4>Overall User Expectation of Verbal commands</h4>

<p>Users expected to use phrases and sentences in their natural language as verbal commands for editing, and they also expected the flexibility of saying a command in multiple ways so they don’t have to learn the command. Words such as “change”, “replace”, “correct” and “delete” were commonly used across all users when changing or correcting a word.</p>

<h4>Overall Tolerance for Error Handling</h4>

<p>Our testing showed that users were quite tolerant of occasional mistakes as they did not expect the system to perfectly get everything right. They only became frustrated when the system repeatedly failed to understand a particular utterance or their commands.</p>

<h2>Takeaway</h2>

<p>The fidelity of behavioral prototype does not necessarily influence the quality of testing. User can be way more engaged into the testing scenario than we expect, if with right instructions. The point is to approach the same goal of learning with minimal effort of building prototyping, which is especially perfect for rapid modification based on result and feedback of each testing.</p>

<p>Although the fidelity does not necessarily matter, a thorough plan is required for the entire process. We planned out the structure and general workflow for our prototyping and testing in advance, in order to make sure our process supported by strong base of theories and analysis. In this way, every possible modification during the iteration of prototyping and testing would all follow the major direction of our plan, which makes work faster, more precise, and easier to facilitate.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Video Prototype: Introducing Supernome App]]></title>
    <link href="http://hcid.us/blog/2014/02/01/video-prototype-assignment-introducing-supernome-app/"/>
    <updated>2014-02-01T22:52:58-08:00</updated>
    <id>http://hcid.us/blog/2014/02/01/video-prototype-assignment-introducing-supernome-app</id>
    <content type="html"><![CDATA[<div class="video-container">
<iframe src="http://hcid.us//player.vimeo.com/video/85534963" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
</div>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Model Prototype: Handheld Device]]></title>
    <link href="http://hcid.us/blog/2014/01/25/model-prototype-assignment-handheld-device/"/>
    <updated>2014-01-25T23:47:39-08:00</updated>
    <id>http://hcid.us/blog/2014/01/25/model-prototype-assignment-handheld-device</id>
    <content type="html"><![CDATA[<h2>Background</h2>

<p><img class="right" src="http://hcid.us/images/prototyping/a2/title.jpg" width="300"></p>

<p>The subject for our second assignment in the prototyping studio, is creating a 3D lo-fi prototype of a handheld device with physical and digital control. We have two product to choose from: an immersion blender or a stud finder. The challenge here is to apply OXO brand and its universal design elements to a new product space. Which means the device I design should be accessible and usable for a wide range of users.</p>

<p>Another challenge for me is that I have never use both equipments in my life before. So that I need to do research in the regular workflow of using these devices and study the current market of both products. After a short research, I decided to go with immersion blender because I can’t only find a blender that is accessible, so I can borrow and play with it.</p>

<h2>Design</h2>

<p><img class="right" src="http://hcid.us/images/prototyping/a2/exploration.png" width="300"></p>

<p>The <a href="http://www.oxo.com/">OXO</a> brand is known for applying universal design to deliver well-designed, easy to use tools for cooking and food preparation. I started with thinking about how to make a handheld blender that can work with different size of hands, and what kind of shape can achieve this goal. So I started to do some sketch like this:</p>

<p>Sketching is an effective way to think with paper, but to design a physical object, it’s much better to think with our hands. So I turned to hold different objects such as water bottle, catch-up bottle, or folding umbrella, and try to use it as an immersion blender. And also by wrapping some tinfoil on the outside of those objects, I can change the shape easily and explore more opportunities within a very short period of time.</p>

<p>After some exploring in different shapes and solutions, I chosed three directions to create a physical model.</p>

<!-- more -->


<h2>Model 1</h2>

<p>The first model is in the shape of a drill. It have two button on the shank to operate the blender to accelerate and decelerate. I use foam to create the whole gun body as it’s extremely easy to create customized shape. The “gunpoint” and barrel is made of wire and foam. And I put some clay inside of the gun body to make it heavier so it feels like there is a real motor inside the object.</p>

<p><img src="http://hcid.us/images/prototyping/a2/model1-sketch.png" width="256"><img src="http://hcid.us/images/prototyping/a2/model1.jpg" width="256"><img src="http://hcid.us/images/prototyping/a2/model1.gif" width="288"></p>

<h2>Model 2</h2>

<p><img class="left" src="http://hcid.us/images/prototyping/a2/catchup.png" width="170">
I design this model based on the idea of catch-up bottle. When using a handheld blender, we always need to move the blender up and down. And it’s important to let user hold the device tightly because of the potential danger when the motor is working. I found the shape of a collapsed catch-up bottle feels good for a blender, so I make the handgrip in this shape and put the switches on the position of the thumb.</p>

<p>The whole body was made of clay and wrapped it with tape. The whisk part is made of a wood stick and some foam wrapped with tinfoil.</p>

<p><img src="http://hcid.us/images/prototyping/a2/model2-sketch.png" width="277"><img src="http://hcid.us/images/prototyping/a2/model2.jpg" width="277"><img src="http://hcid.us/images/prototyping/a2/model2.gif" width="246"></p>

<h2>Model 3</h2>

<p>This model also use the catch-up bottle approach. And I made it standable with the button. You may ask why I need a blender to stand on the opposite way. The reason is that after blending some food, the whisk part will get dirty and maybe some juices were sticked to it. If you put the dirty part on the table, both table and the whisk part may become dirty. I granulated the neck part to prevent the juices flow down to the handgrip. For this model, the body is made of clay and the neck part is made of foam.</p>

<p><img src="http://hcid.us/images/prototyping/a2/model3-sketch.png" width="272"><img src="http://hcid.us/images/prototyping/a2/model3.jpg" width="272"><img src="http://hcid.us/images/prototyping/a2/model3.gif" width="256"></p>

<h2>Test &amp; Analysis</h2>

<p>Many problems are revealed during the test. For the model 1, I found it’s much harder to operate it for short people unless you put the container on a tea table. The gun shape decided that it’s easier to push down form a higher angle.</p>

<p>For model 2, it’s hard to operate the switch button for people who have small hands. Ideally the user should be able to operate the blender with one hand, but with this model, people with small hands may need a second hand to change the switches.</p>

<p>The design of model 3 is in cone-shape which is easy to hold for different hand size. But the problem is that in making this physical prototype, I didn’t control the size well and made it much larger than what I thought. So for people with smaller hand it’s hard to reach the control button.</p>

<p>Here is an one minute clip of the usability testing with these three model prototypes:</p>

<div class="video-container">
<iframe src="http://hcid.us//player.vimeo.com/video/84949884" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
</div>


<h2>Reflection</h2>

<p>When designing physical device, it’s easier to think with our hands to see what works. Even holding a simple object can help with designing the shape. And with a little effort, you can try out many different approaches in a second which help me to narrow down my choices rapidly.</p>

<p>I found that Lo-fi 3d prototype provide a more concrete context for interaction designers to think about the interaction with a physical object. And it’s also a good practice to think human computer interaction in a manner of human factors and ergonomics.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Model Prototype Exercise: ecoATM]]></title>
    <link href="http://hcid.us/blog/2014/01/21/model-prototype-exercise-ecoatm/"/>
    <updated>2014-01-21T22:42:27-08:00</updated>
    <id>http://hcid.us/blog/2014/01/21/model-prototype-exercise-ecoatm</id>
    <content type="html"><![CDATA[<div class="video-container">
<iframe src="http://hcid.us//player.vimeo.com/video/84655899" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
</div>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Paper Prototype: Wearable UIs]]></title>
    <link href="http://hcid.us/blog/2014/01/21/paper-prototype-assignment-wearable-uis/"/>
    <updated>2014-01-21T17:49:25-08:00</updated>
    <id>http://hcid.us/blog/2014/01/21/paper-prototype-assignment-wearable-uis</id>
    <content type="html"><![CDATA[<h2>Design</h2>

<p><img class="right" src="http://hcid.us/images/prototyping/a1/metronome.jpg" width="160">
For this paper prototype assignment, I designed a metronome application that works on both mobile phone and smart watch. It helps to improve musical performance like a traditional metronome while wearable device enabled us to create an even better user experience.</p>

<p>In the picture is a mechanical wind-up metronome which can produce a steady beats: “Da Da Da Da &hellip;”. The consistent tempo help internalizing a clear sense of timing and tempo while musician is playing. By adjusting the metronome, the device can produce varying tempi, usually ranging from 40 to 208 BPM(Beats Per Minutes).</p>

<p>Existing software metronomes and electronic metronomes usually produce the tempo by sound. The problem with using sound is that the sound may be interruptive for audience, and it’s hard to follow the sound on stage(noisy environment).</p>

<p>One thing I found most excited about using Pebble is vibrating on wrist. It enabled a new dimension of sensing information from a digital device without actively paying attention to the device. My idea is by setting the smartwatch to vibrate in a certain tempo, to give musician a more strong sense of the beats.</p>

<!-- more -->


<p><img class="left" src="http://hcid.us/images/prototyping/a1/sketch.JPG" width="320"></p>

<p>I sketched my first design on whiteboard. It uses the mobile phone interface to set the tempo, while user can also adjust the tempo directly from the watch.</p>

<p>There are two ways to change the BPM value from the phone. One is by spinning the wheel in the middle of the screen, and the other one is by continuously tapping on the button to give an approximate speed. The first method is more accurate and the second one is more intuitive and more useful for composers.
The wheel is easy to understand, it’s like a metaphor of physical tuner. But I’m not sure about the “tapping” part. So I start to make a paper prototype and test this design.</p>

<h2>Prototype</h2>

<p>I build the first version of this prototype within 10 minutes. It’s a quick and dirty one, with only the necessary components on one white paper. To operate this prototype, I need to change the BPM value by writing on a small sticky note and change it while the user using the prototype.</p>

<p>I made a watch face and attach it to a Pebble watch so that the user can actually wear the watch and try the interactions in a more realistic setting. Once the user click start on the phone, the smart watch will launch this app which allow the user to adjust the tempo by the “up/down” button, and exit the app through the “Home” button.</p>

<p><img class="left" src="http://hcid.us/images/prototyping/a1/p1.1.jpg" width="320">
<img src="http://hcid.us/images/prototyping/a1/p1.2.jpg" width="320"></p>

<p>As some effects can’t be implemented with paper prototype, I will just tell them what happens on the interface or the devices. That includes slight changes on the BPM number, the vibrating of the watch, the sound tempo make by the phone, and the backlight feedback for clicking the tap button.</p>

<p>After several tests, I build a second version based on the feedbacks from the tests with the first paper prototype.</p>

<p>With the second paper prototype, I use cardboard to create a base in the shape of an iPhone so that the user can actually hold the phone on hand. I use a scrolling bar to change the BPM value so that the feedback from tapping and wheel could be faster in the testing. And the scrolling up and down actually simulate the animation I want for this interface. While the user is adjusting the wheel, I will scroll the bar up or down according to the user’s operation. In the first time the user touch the “Tap” button, a pop-up dialog will show up to tell the user how to use this button.</p>

<p><img class="left" src="http://hcid.us/images/prototyping/a1/p2.1.JPG" width="320">
<img src="http://hcid.us/images/prototyping/a1/p2.2.jpg" width="320"></p>

<h2>Testing &amp; Analysis</h2>

<p>This is a minute clip presenting the usability testing for the 2 iterations of my paper prototype:</p>

<div class="video-container">
<iframe src="http://hcid.us//player.vimeo.com/video/84655291" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
</div>


<p>During the tests with the first prototype, I found that the text “Try tap your tempo” on the left side is extremely helpful for the user to understand how to use it. I tried to remove it and test it with another user, and it failed in the tapping part. The user can’t understand how to use the Tap button without this simple instruction. But I also noticed that this text instruction is unnecessary after the user figure out how to use it. So I change it to a dialog format.</p>

<p>On the other hand, spinning the wheel is more like a self explaining UI component, user can easily figure out how to use it. That may because it looks just like a rotary knob, and it’s natural for it to change the number above it. And this metaphor is widely used in many software interface for turning volume, adjusting parameters and also hardwares such as the classic iPod.</p>

<p>I found that changing the number instantly (or faster) will largely help the user to understand the UI, and replacing the sticky notes is too time-consuming in the test. That’s why I use a scrolling bar for changing the BPM value in the improved version, as it’s just an increasing array of numbers.</p>

<p>Another problem I found in the testing is that user barely use the interface on smartwatch to adjust the tempo. Most of the time they just use the mobile phone to adjust the tempo and to start/stop the beats, although most of them like the idea of vibrating on smartwatch. On a second thought, this app can totally work without the mobile phone: adjusting one number is easy enough to operate on a watch with a simple interface like the one I designed. Moreover if the smartwatch have a motion sensor and microphone, it’s possible to ask the user to set the tempo by snapping their finger which is even more fun to use.</p>

<p>Beside adjusting the tempo, the phone actually allow us to do more complicate operations. For example, the user can choose a song from the music library and let the watch vibrate in a rhythmic beats, or use the phone to connect and sync the tempo among multiple smart watches, which may be useful for a band to collaborate.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Paper Prototype Exercise: Coffee Ordering Tablet App]]></title>
    <link href="http://hcid.us/blog/2014/01/21/paper-prototype-exercise-coffee-ordering-tablet-app/"/>
    <updated>2014-01-21T17:49:01-08:00</updated>
    <id>http://hcid.us/blog/2014/01/21/paper-prototype-exercise-coffee-ordering-tablet-app</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://hcid.us/images/prototyping/e1/overview.JPG" width="320"></p>

<p>In the first week of our prototyping class, we explored paper prototyping through a quick exercise: to design a simple coffee ordering tablet app that allowed users to order three different types of coffee in three different sizes.</p>

<p>We have to make a paper prototype of our design very quickly and then test the paper prototype with users and evaluate the design.</p>

<p>We started with sketching out our ideas for the UI independently and then gathered to critique and discuss different approaches. Then we quickly build our first paper prototype, and test it with any people around that are available.</p>

<p>The usability testing and the feedbacks from the users helped us to find some design flaws. After multiple iterations, we made a lot of improvements to our initial design and the final version of our prototype have much better performance in the usability testing.</p>

<p><img src="http://hcid.us/images/prototyping/e1/1.jpg" width="160"><img src="http://hcid.us/images/prototyping/e1/2.jpg" width="160"><img src="http://hcid.us/images/prototyping/e1/3.jpg" width="160"><img src="http://hcid.us/images/prototyping/e1/4.jpg" width="160"><img src="http://hcid.us/images/prototyping/e1/5.jpg" width="160"></p>
]]></content>
  </entry>
  
</feed>
